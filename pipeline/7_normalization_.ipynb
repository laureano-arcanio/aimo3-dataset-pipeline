{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import json\n",
    "import re\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration/home/larcanio/AIMO3_v2/data/datasets/splits/bucked_2_3_splitt/test_difficulty_level.jsonl\n",
    "OUTPUT_BASE_DIR = Path('/home/larcanio/AIMO3_v2/data/datasets/splits/')\n",
    "RUN_DIR = OUTPUT_BASE_DIR / 'bucked_2_3_splitt/'\n",
    "DATASET_NAME = 'Numina1.5'\n",
    "LICENSE = 'Apache 2.0'\n",
    "INPUT_FILENAME = 'test_difficulty_level.jsonl'\n",
    "OUTPUT_FILENAME = 'train_dataset_normalized.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using specified run directory: bucked_2_3_splitt\n",
      "Reading from: /home/larcanio/AIMO3_v2/data/datasets/splits/bucked_2_3_splitt/test_difficulty_level.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Find the JSONL file\n",
    "if RUN_DIR is None:\n",
    "    # Find the most recent run directory\n",
    "    run_dirs = sorted([d for d in OUTPUT_BASE_DIR.iterdir() if d.is_dir()], reverse=True)\n",
    "    if not run_dirs:\n",
    "        raise FileNotFoundError(f\"No run directories found in {OUTPUT_BASE_DIR}\")\n",
    "    RUN_DIR = run_dirs[0]\n",
    "    print(f\"Using most recent run directory: {RUN_DIR.name}\")\n",
    "else:\n",
    "    print(f\"Using specified run directory: {RUN_DIR.name}\")\n",
    "\n",
    "jsonl_file = RUN_DIR / INPUT_FILENAME\n",
    "\n",
    "if not jsonl_file.exists():\n",
    "    raise FileNotFoundError(f\"JSONL file not found: {jsonl_file}\")\n",
    "\n",
    "print(f\"Reading from: {jsonl_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total datapoints loaded: 1271\n",
      "Displaying first 10 items:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read and parse JSONL file\n",
    "datapoints = []\n",
    "with open(jsonl_file, 'r', encoding='utf-8') as f:\n",
    "    for line_num, line in enumerate(f, 1):\n",
    "        if line.strip():  # Skip empty lines\n",
    "            try:\n",
    "                datapoint = json.loads(line)\n",
    "                datapoints.append(datapoint)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error parsing line {line_num}: {e}\")\n",
    "                print(f\"Line content: {line[:200]}...\")\n",
    "\n",
    "NUM_ITEMS_TO_DISPLAY = 10\n",
    "print(f\"\\nTotal datapoints loaded: {len(datapoints)}\")\n",
    "print(f\"Displaying first {min(NUM_ITEMS_TO_DISPLAY, len(datapoints))} items:\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identity & provenance\n",
    "# id -> create uuid for this dataset\n",
    "# source_dataset -> dataset (one of [Numina1.5, OpenMath, GSM8k])\n",
    "# source_problem_id -> problem_id\n",
    "# source_license -> [Numina1.5: Apache 2.0, OpenMath: CC-BY-SA 4.0, GSM8k: MIT]\n",
    "\n",
    "# Problem & answers\n",
    "# text -> problem.text\n",
    "# answer_expected -> problem.expected_answer\n",
    "# answer_predicted -> outcome.answer\n",
    "# is_correct -> attempts[where it succeeds].result.correct\n",
    "\n",
    "# Program (the actual training payload)\n",
    "# code -> attempts[where it succeeds].code\n",
    "# code_runtime_ms -> attempts[where it succeeds].exec.duration  (convert from seconds to ms)\n",
    "# code_generated_tokens -> (compute using tokenizer)\n",
    "\n",
    "# Curriculum labels\n",
    "# domain -> audit.2nd_stage_domain, fallback to audit.domain\n",
    "# reasoning_tier -> derived from difficulty_buckets (bucket_1=1 .. bucket_5=5)\n",
    "\n",
    "# Quality + filtering knobs\n",
    "# tier -> audit.tier (rename: keep to core, keep_with_flags to extended, else drop)\n",
    "# code_score -> audit.code_score (fallback: audit.scores.overall)\n",
    "# risk_flags -> audit.risk_flags or audit.flags; brute_force -> enumerative_strategy\n",
    "\n",
    "# Model provenance\n",
    "# generation_model -> models.generating\n",
    "# audit_model -> record.audit_model or audit.audit_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tier mapping: old names (keep/keep_with_flags) and already-normalized (core/extended) -> output tier; else -> drop\n",
    "TIER_MAP = {\"keep\": \"core\", \"keep_with_flags\": \"extended\", \"core\": \"core\", \"extended\": \"extended\"}\n",
    "\n",
    "def _successful_attempt(record):\n",
    "    \"\"\"First attempt with exec.status == 'success'.\"\"\"\n",
    "    for a in record.get(\"attempts\", []):\n",
    "        if (a.get(\"exec\") or {}).get(\"status\") == \"success\":\n",
    "            return a\n",
    "    return None\n",
    "\n",
    "def _problem_id_from_record(record):\n",
    "    \"\"\"Source problem id: record['problem_id'] or parse from record['id'].\"\"\"\n",
    "    if record.get(\"problem_id\") is not None:\n",
    "        return str(record[\"problem_id\"])\n",
    "    raw_id = record.get(\"id\") or \"\"\n",
    "    for part in raw_id.split(\"_\"):\n",
    "        if part.isdigit() and len(part) < 10:\n",
    "            return part\n",
    "    return raw_id or None\n",
    "\n",
    "def _risk_flags(audit):\n",
    "    \"\"\"audit.risk_flags or audit.flags; 'brute_force' renamed to 'enumerative_strategy'.\"\"\"\n",
    "    a = audit or {}\n",
    "    flags = list(a.get(\"risk_flags\") or a.get(\"flags\") or [])\n",
    "    return [\"enumerative_strategy\" if f == \"brute_force\" else f for f in flags]\n",
    "\n",
    "def _normalize_code_comment_prefixes(code):\n",
    "    \"\"\"Replace # Goal: -> # Objective: and # Plan: -> # Approach: in code (for stored code property).\"\"\"\n",
    "    if not code or not isinstance(code, str):\n",
    "        return code or \"\"\n",
    "    return code.replace(\"# Goal:\", \"# Objective:\").replace(\"# Plan:\", \"# Approach:\")\n",
    "\n",
    "def _strip_comment_value(text):\n",
    "    \"\"\"Remove leading ': ' or ':' from extracted objective/approach (e.g. '# Approach: : Enumerate...').\"\"\"\n",
    "    if not text:\n",
    "        return text\n",
    "    t = text.strip()\n",
    "    if t.startswith(\": \"):\n",
    "        return t[2:].strip()\n",
    "    if t.startswith(\":\"):\n",
    "        return t[1:].strip()\n",
    "    return t\n",
    "\n",
    "def _extract_objective_and_approach(code):\n",
    "    \"\"\"\n",
    "    Extract # Objective: and # Approach: comment lines from code (first occurrence of each).\n",
    "    Strips any leading ': ' from the value. Returns (objective, approach); missing values are \"\".\n",
    "    \"\"\"\n",
    "    if not code or not isinstance(code, str):\n",
    "        return \"\", \"\"\n",
    "    objective, approach = \"\", \"\"\n",
    "    for line in code.splitlines():\n",
    "        s = line.strip()\n",
    "        if s.startswith(\"# Objective:\"):\n",
    "            if objective == \"\":\n",
    "                objective = _strip_comment_value(s[12:])  # len(\"# Objective:\") == 12\n",
    "        elif s.startswith(\"# Approach:\"):\n",
    "            if approach == \"\":\n",
    "                approach = _strip_comment_value(s[11:])  # len(\"# Approach:\") == 11\n",
    "    return objective, approach\n",
    "\n",
    "\n",
    "def _reasoning_tier(record):\n",
    "    \"\"\"\n",
    "    Derive reasoning_tier (1-5) from difficulty_buckets.\n",
    "    Returns the bucket number for the first True bucket, or None if unclassified.\n",
    "    \"\"\"\n",
    "    buckets = record.get(\"difficulty_buckets\") or {}\n",
    "    for tier in range(1, 6):\n",
    "        if buckets.get(f\"bucket_{tier}\") is True:\n",
    "            return tier\n",
    "    return None\n",
    "\n",
    "\n",
    "FLOAT_REGEX = re.compile(\n",
    "    r\"\"\"\n",
    "    (?<!\\w)          # not part of identifier\n",
    "    \\d+\\.\\d+         # decimal literal\n",
    "    |                # or\n",
    "    float\\s*\\(       # float(...)\n",
    "    \"\"\",\n",
    "    re.VERBOSE,\n",
    ")\n",
    "\n",
    "\n",
    "class CodeFeatureExtractor(ast.NodeVisitor):\n",
    "    def __init__(self):\n",
    "        self.features = set()\n",
    "        self.loop_depth = 0\n",
    "        self.max_loop_depth = 0\n",
    "\n",
    "    def visit_Import(self, node):\n",
    "        for alias in node.names:\n",
    "            name = alias.name.split(\".\")[0]\n",
    "            if name == \"sympy\":\n",
    "                self.features.add(\"uses_sympy\")\n",
    "            elif name == \"itertools\":\n",
    "                self.features.add(\"uses_itertools\")\n",
    "            elif name == \"fractions\":\n",
    "                self.features.add(\"uses_fractions\")\n",
    "            elif name == \"numpy\":\n",
    "                self.features.add(\"uses_numpy\")\n",
    "        self.generic_visit(node)\n",
    "\n",
    "    def visit_ImportFrom(self, node):\n",
    "        if node.module:\n",
    "            root = node.module.split(\".\")[0]\n",
    "            if root == \"sympy\":\n",
    "                self.features.add(\"uses_sympy\")\n",
    "            elif root == \"itertools\":\n",
    "                self.features.add(\"uses_itertools\")\n",
    "            elif root == \"fractions\":\n",
    "                self.features.add(\"uses_fractions\")\n",
    "            elif root == \"numpy\":\n",
    "                self.features.add(\"uses_numpy\")\n",
    "        self.generic_visit(node)\n",
    "\n",
    "    def visit_FunctionDef(self, node):\n",
    "        self.features.add(\"uses_functions\")\n",
    "        self.generic_visit(node)\n",
    "\n",
    "    def visit_For(self, node):\n",
    "        self._enter_loop()\n",
    "        self.generic_visit(node)\n",
    "        self._exit_loop()\n",
    "\n",
    "    def visit_While(self, node):\n",
    "        self._enter_loop()\n",
    "        self.generic_visit(node)\n",
    "        self._exit_loop()\n",
    "\n",
    "    def _enter_loop(self):\n",
    "        self.features.add(\"has_loops\")\n",
    "        self.loop_depth += 1\n",
    "        self.max_loop_depth = max(self.max_loop_depth, self.loop_depth)\n",
    "\n",
    "    def _exit_loop(self):\n",
    "        self.loop_depth -= 1\n",
    "\n",
    "    def visit_Assert(self, node):\n",
    "        self.features.add(\"has_asserts\")\n",
    "        self.generic_visit(node)\n",
    "\n",
    "\n",
    "def extract_code_features(code: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extract deterministic code feature tags from Python code.\n",
    "    Returns a sorted list of feature strings.\n",
    "    \"\"\"\n",
    "    extractor = CodeFeatureExtractor()\n",
    "    try:\n",
    "        tree = ast.parse(code)\n",
    "        extractor.visit(tree)\n",
    "    except SyntaxError:\n",
    "        return []\n",
    "    if FLOAT_REGEX.search(code):\n",
    "        extractor.features.add(\"uses_floats\")\n",
    "    if extractor.max_loop_depth >= 2:\n",
    "        extractor.features.add(\"uses_deep_loops\")\n",
    "    return sorted(extractor.features)\n",
    "\n",
    "\n",
    "def record_to_flat(record, dataset_name, license_, tokenizer_fn=None):\n",
    "    \"\"\"\n",
    "    Convert one source record to the normalized flat format.\n",
    "    Returns None only if there is no successful attempt.\n",
    "    domain: audit.2nd_stage_domain with fallback to audit.domain. tokenizer_fn(code: str) -> int | None (optional).\n",
    "    \"\"\"\n",
    "    attempt = _successful_attempt(record)\n",
    "    if attempt is None:\n",
    "        return None\n",
    "\n",
    "    problem = record.get(\"problem\") or {}\n",
    "    audit = record.get(\"audit\") or {}\n",
    "    decision = (audit.get(\"tier\") or \"\").strip().lower()\n",
    "    classification = audit.get(\"classification\") or {}\n",
    "\n",
    "    # domain: audit.2nd_stage_domain with fallback to audit.domain\n",
    "    domain = audit.get(\"2nd_stage_domain\") or audit.get(\"domain\")\n",
    "    domain = domain.strip() if isinstance(domain, str) else domain\n",
    "    models = record.get(\"models\") or {}\n",
    "    exec_ = attempt.get(\"exec\") or {}\n",
    "    result = attempt.get(\"result\") or {}\n",
    "\n",
    "    # answer_predicted: from successful attempt's result.predicted (outcome.answer not in source)\n",
    "    answer_predicted = result.get(\"predicted\")\n",
    "\n",
    "    # code_runtime_ms: exec.duration is in seconds\n",
    "    duration_sec = exec_.get(\"duration\")\n",
    "    code_runtime_ms = int(duration_sec * 1000) if isinstance(duration_sec, (int, float)) else None\n",
    "\n",
    "    # code: normalize comment prefixes (# Goal: -> # Objective:, # Plan: -> # Approach:) then use for storage and extraction\n",
    "    code = attempt.get(\"code\") or \"\"\n",
    "    code = _normalize_code_comment_prefixes(code)\n",
    "    code_generated_tokens = tokenizer_fn(code) if tokenizer_fn else None\n",
    "\n",
    "    # objective and approach from # Objective: / # Approach: comments in code\n",
    "    objective, approach = _extract_objective_and_approach(code)\n",
    "\n",
    "    # code_features: deterministic list of features present in the code\n",
    "    code_features = extract_code_features(code)\n",
    "\n",
    "    # tier: keep/core, keep_with_flags/extended, or drop (all tiers kept in output)\n",
    "    # Default to \"drop\" for empty/missing tier or any unmapped value\n",
    "    tier = TIER_MAP.get(decision, \"drop\")\n",
    "\n",
    "    # Preserve original dataset from record when present; otherwise use dataset_name\n",
    "    dataset_value = record.get(\"dataset\") or dataset_name\n",
    "\n",
    "    # text_tokens: count tokens in problem text\n",
    "    problem_text = problem.get(\"text\")\n",
    "    text_tokens = tokenizer_fn(problem_text) if tokenizer_fn and problem_text else None\n",
    "\n",
    "    # solution: original_solution from source record (if present)\n",
    "    solution = record.get(\"original_solution\")\n",
    "\n",
    "    # reasoning_tier: derived from difficulty_buckets (1-5)\n",
    "    reasoning_tier = _reasoning_tier(record)\n",
    "\n",
    "    flat = {\n",
    "        # ─────────────────────────────────────────────\n",
    "        # 1. Identity & provenance\n",
    "        # ─────────────────────────────────────────────\n",
    "        \"id\": str(uuid.uuid4()),\n",
    "        \"dataset\": dataset_value,\n",
    "        \"problem_id\": _problem_id_from_record(record),\n",
    "        \"license\": license_,\n",
    "\n",
    "        # ─────────────────────────────────────────────\n",
    "        # 2. Problem definition\n",
    "        # ─────────────────────────────────────────────\n",
    "        \"domain\": domain,\n",
    "        \"reasoning_tier\": reasoning_tier,\n",
    "        \"tier\": tier,\n",
    "        \"text\": problem_text,\n",
    "        \"text_tokens\": text_tokens,\n",
    "        \"solution\": solution,\n",
    "        \"answer_expected\": problem.get(\"expected_answer\"),\n",
    "\n",
    "        # ─────────────────────────────────────────────\n",
    "        # 3. Generated solution (semantic)\n",
    "        # ─────────────────────────────────────────────\n",
    "        \"objective\": objective,\n",
    "        \"approach\": approach,\n",
    "        \"answer_predicted\": answer_predicted,\n",
    "\n",
    "        # ─────────────────────────────────────────────\n",
    "        # 4. Verification & correctness\n",
    "        # ─────────────────────────────────────────────\n",
    "        \"is_correct\": result.get(\"correct\"),\n",
    "\n",
    "        # ─────────────────────────────────────────────\n",
    "        # 5. Code artifact\n",
    "        # ─────────────────────────────────────────────\n",
    "        \"code\": code or None,\n",
    "        \"code_features\": code_features,\n",
    "\n",
    "        # ─────────────────────────────────────────────\n",
    "        # 6. Code execution telemetry\n",
    "        # ─────────────────────────────────────────────\n",
    "        \"code_runtime_ms\": code_runtime_ms,\n",
    "        \"code_generated_tokens\": code_generated_tokens,\n",
    "\n",
    "        # ─────────────────────────────────────────────\n",
    "        # 7. Quality, audit & risk\n",
    "        # ─────────────────────────────────────────────\n",
    "        \"code_score\": audit.get(\"code_score\") if \"code_score\" in (audit or {}) else audit.get(\"scores\", {}).get(\"overall\"),\n",
    "        \"risk_flags\": _risk_flags(audit),\n",
    "\n",
    "        # ─────────────────────────────────────────────\n",
    "        # 8. Model lineage\n",
    "        # ─────────────────────────────────────────────\n",
    "        \"generation_model\": models.get(\"generating\"),\n",
    "        \"audit_model\": record.get(\"audit_model\") or audit.get(\"audit_model\"),\n",
    "    }\n",
    "    return flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "_tokenizer_fn = None\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "_tokenizer_fn = lambda code: len(enc.encode(code, disallowed_special=())) if code else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read: 1271 records from /home/larcanio/AIMO3_v2/data/datasets/splits/bucked_2_3_splitt/test_difficulty_level.jsonl\n",
      "Written: 1271 records to /home/larcanio/AIMO3_v2/data/datasets/splits/bucked_2_3_splitt/train_dataset_normalized.jsonl\n",
      "Skipped (no successful attempt): 0\n"
     ]
    }
   ],
   "source": [
    "# Convert and write normalized output (read-only: original dataset is not modified)\n",
    "out_path = RUN_DIR / OUTPUT_FILENAME\n",
    "normalized = []\n",
    "for rec in datapoints:\n",
    "    flat = record_to_flat(rec, DATASET_NAME, LICENSE, tokenizer_fn=_tokenizer_fn)\n",
    "    if flat is not None:\n",
    "        normalized.append(flat)\n",
    "\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for rec in normalized:\n",
    "        f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Read: {len(datapoints)} records from {jsonl_file}\")\n",
    "print(f\"Written: {len(normalized)} records to {out_path}\")\n",
    "print(f\"Skipped (no successful attempt): {len(datapoints) - len(normalized)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"94ab8f03-a8e3-482e-a24c-3a7cea9a93ec\",\n",
      "  \"dataset\": \"numina1.5\",\n",
      "  \"problem_id\": \"1855\",\n",
      "  \"license\": \"Apache 2.0\",\n",
      "  \"domain\": \"algebra\",\n",
      "  \"reasoning_tier\": null,\n",
      "  \"tier\": \"core\",\n",
      "  \"text\": \"I am thinking of a three-digit natural number less than 200. If I round its triple to the hundreds, it increases by 36. Which number am I thinking of?\\n\\n(M. Dillingerová)\",\n",
      "  \"text_tokens\": 44,\n",
      "  \"solution\": null,\n",
      "  \"answer_expected\": \"188\",\n",
      "  \"objective\": \"Find the unique three‑digit natural number x (<200) such that its triple 3x, rounded to the nearest hundred, equals 3x + 36.\",\n",
      "  \"approach\": \"Translate the rounding condition into a congruence modulo 100 and enumerate the possible x in the given range. Verify the result with an assertion that the rounding rule holds.\",\n",
      "  \"answer_predicted\": \"188\",\n",
      "  \"is_correct\": true,\n",
      "  \"code\": \"# Objective: Find the unique three‑digit natural number x (<200) such that its triple 3x, rounded to the nearest hundred, equals 3x + 36.\\n# Approach: Translate the rounding condition into a congruence modulo 100 and enumerate the possible x in the given range. Verify the result with an assertion that the rounding rule holds.\\n\\nfrom fractions import Fraction\\n\\n# The rounding rule says 3x rounded to the nearest hundred equals (3x + 36).\\n# For integer 3x, rounding to the nearest hundred means adding the minimal amount to reach a multiple of 100.\\n# Thus (3x + 36) must be a multiple of 100 and 3x must be exactly 36 less than that multiple.\\n# This gives the congruence 3x ≡ 64 (mod 100).\\ndef find_x():\\n    candidates = []\\n    for x in range(100, 200):          # x is three‑digit and <200\\n        if (3 * x) % 100 == 64:        # congruence condition\\n            candidates.append(x)\\n    assert len(candidates) == 1, \\\"There is exactly one valid candidate.\\\"\\n    x = candidates[0]\\n    # Verify that rounding 3x to the nearest hundred indeed yields 3x+36\\n    rounded = (3*x + 99)//100*100            # integer division rounds down to the nearest hundred\\n    assert rounded == 3*x + 36, \\\"Rounding condition verified.\\\"\\n    return x\\n\\nprint(find_x())\",\n",
      "  \"code_features\": [\n",
      "    \"has_asserts\",\n",
      "    \"has_loops\",\n",
      "    \"uses_fractions\",\n",
      "    \"uses_functions\"\n",
      "  ],\n",
      "  \"code_runtime_ms\": 90,\n",
      "  \"code_generated_tokens\": 327,\n",
      "  \"code_score\": 5,\n",
      "  \"risk_flags\": [],\n",
      "  \"generation_model\": \"gpt-oss\",\n",
      "  \"audit_model\": \"gpt-oss\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Preview first normalized record\n",
    "if normalized:\n",
    "    print(json.dumps(normalized[1], indent=2, ensure_ascii=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
