{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "6e6ea54c",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import sqlite3\n",
        "from pathlib import Path\n",
        "from tqdm.notebook import tqdm\n",
        "import io\n",
        "import contextlib\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "import anyio\n",
        "import httpx\n",
        "from utils import LLMPool, RuntimeConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "498d1c92",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[config] reloaded: MAX_CONCURRENT_REQUESTS: 5 -> 30\n",
            "RuntimeConfig(MAX_EXECUTION_RETRIES=3, MAX_ANSWER_RETRIES=3, LLM_REQUEST_RETRY_COUNT=3, LLM_REQUEST_TIMEOUT_SECONDS=300, EXECUTION_TIMEOUT_SECONDS=30, MAX_CONCURRENT_REQUESTS=30)\n"
          ]
        }
      ],
      "source": [
        "# Configuration\n",
        "API_KEY = \"sk-local\"\n",
        "MODEL_NAME = \"gpt-oss\"\n",
        "SYSTEM_MESSAGE = \"You are a mathematician writing Python code to solve problems.\"\n",
        "NGINX_BALANCER_URL = \"http://127.0.0.1:8080/v1\"\n",
        "\n",
        "TEMPERATURE = 1\n",
        "MAX_TOKENS = 1024 * 20\n",
        "REASONING_EFFORT = \"low\"\n",
        "\n",
        "OUTPUT_BASE_DIR = Path('/home/larcanio/AIMO3_v2/data/datasets/mvidia_reasoning_steps/DeepSeek-R1')\n",
        "PROMPT_VERSION = \"v1\"\n",
        "MAX_PROBLEMS_TO_PROCESS = None\n",
        "\n",
        "# Hot-reloadable settings (edit config.json during run)\n",
        "CONFIG_FILE = \"config.json\"\n",
        "\n",
        "cfg = RuntimeConfig(CONFIG_FILE, defaults={\n",
        "    \"MAX_EXECUTION_RETRIES\": 3,\n",
        "    \"MAX_ANSWER_RETRIES\": 3,\n",
        "    \"LLM_REQUEST_RETRY_COUNT\": 3,\n",
        "    \"LLM_REQUEST_TIMEOUT_SECONDS\": 300,\n",
        "    \"EXECUTION_TIMEOUT_SECONDS\": 30,\n",
        "    \"MAX_CONCURRENT_REQUESTS\": 5,\n",
        "})\n",
        "print(f\"Configuration loaded: {cfg}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "ab9c5134",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Create SQLite database\n",
        "# db_path = Path('/home/larcanio/AIMO3_v2/mvidia_reasoning_steps.db')\n",
        "# db_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# # Connect to database\n",
        "# conn = sqlite3.connect(str(db_path))\n",
        "# cursor = conn.cursor()\n",
        "\n",
        "# # Create table with schema based on dataset description\n",
        "# cursor.execute('''\n",
        "# CREATE TABLE IF NOT EXISTS reasoning_steps (\n",
        "#     id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "#     problem TEXT NOT NULL,\n",
        "#     generated_solution TEXT,\n",
        "#     generation_model TEXT,\n",
        "#     problem_type TEXT,\n",
        "#     expected_answer TEXT,\n",
        "#     problem_source TEXT,\n",
        "#     inference_mode TEXT,\n",
        "#     pass_rate_72b_tir TEXT,\n",
        "#     used_in_kaggle TEXT,\n",
        "#     created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
        "# )\n",
        "# ''') \n",
        "\n",
        "# # Create index on commonly queried fields\n",
        "# cursor.execute('CREATE INDEX IF NOT EXISTS idx_problem_type ON reasoning_steps(problem_type)')\n",
        "# cursor.execute('CREATE INDEX IF NOT EXISTS idx_generation_model ON reasoning_steps(generation_model)')\n",
        "# cursor.execute('CREATE INDEX IF NOT EXISTS idx_inference_mode ON reasoning_steps(inference_mode)')\n",
        "# cursor.execute('CREATE INDEX IF NOT EXISTS idx_problem_source ON reasoning_steps(problem_source)')\n",
        "# cursor.execute('CREATE INDEX IF NOT EXISTS idx_used_in_kaggle ON reasoning_steps(used_in_kaggle)')\n",
        "\n",
        "# conn.commit()\n",
        "# print(\"Database and table created successfully\")\n",
        "# # \n",
        "# # Load all parquet files from data/nvidia/data\n",
        "# data_dir = Path('/home/larcanio/AIMO3_v2/data/nvidia/data')\n",
        "# parquet_files = sorted(glob.glob(str(data_dir / '*.parquet')))\n",
        "\n",
        "# print(f\"\\nFound {len(parquet_files)} parquet files\")\n",
        "\n",
        "# # Load and insert all parquet files into SQLite database\n",
        "# total_inserted = 0\n",
        "# for parquet_file in tqdm(parquet_files, desc=\"Processing files\", total=len(parquet_files)):\n",
        "#     df = pd.read_parquet(parquet_file)\n",
        "    \n",
        "#     # Map column names to database schema\n",
        "#     # Handle different possible column names in parquet files\n",
        "#     insert_data = []\n",
        "#     for _, row in tqdm(df.iterrows(), desc=\"Processing rows\", total=len(df  )):\n",
        "#         # Map common column variations to our schema\n",
        "#         record = {\n",
        "#             'problem': row.get('problem') or row.get('question', ''),\n",
        "#             'generated_solution': row.get('generated_solution') or row.get('solution') or row.get('answer', ''),\n",
        "#             'generation_model': row.get('generation_model', ''),\n",
        "#             'problem_type': row.get('problem_type', ''),\n",
        "#             'expected_answer': row.get('expected_answer', ''),\n",
        "#             'problem_source': row.get('problem_source', ''),\n",
        "#             'inference_mode': row.get('inference_mode', ''),\n",
        "#             'pass_rate_72b_tir': row.get('pass_rate_72b_tir', 'n/a'),\n",
        "#             'used_in_kaggle': row.get('used_in_kaggle', '')\n",
        "#         }\n",
        "#         insert_data.append(record)\n",
        "    \n",
        "#     # Insert data into database\n",
        "#     cursor.executemany('''\n",
        "#         INSERT INTO reasoning_steps \n",
        "#         (problem, generated_solution, generation_model, problem_type, \n",
        "#          expected_answer, problem_source, inference_mode, pass_rate_72b_tir, used_in_kaggle)\n",
        "#         VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
        "#     ''', [\n",
        "#         (r['problem'], r['generated_solution'], r['generation_model'], r['problem_type'],\n",
        "#          r['expected_answer'], r['problem_source'], r['inference_mode'], r['pass_rate_72b_tir'], r['used_in_kaggle'])\n",
        "#         for r in insert_data\n",
        "#     ])\n",
        "    \n",
        "#     inserted_count = len(insert_data)\n",
        "#     total_inserted += inserted_count\n",
        "#     conn.commit()\n",
        "#     print(f\"Inserted {inserted_count} rows from {Path(parquet_file).name}\")\n",
        "\n",
        "# print(f\"\\nTotal inserted: {total_inserted} examples\")\n",
        "\n",
        "# # Verify the data\n",
        "# cursor.execute('SELECT COUNT(*) FROM reasoning_steps')\n",
        "# total_count = cursor.fetchone()[0]\n",
        "# print(f\"Total records in database: {total_count}\")\n",
        "\n",
        "# # Show sample entry\n",
        "# cursor.execute('SELECT * FROM reasoning_steps LIMIT 1')\n",
        "# sample = cursor.fetchone()\n",
        "# if sample:\n",
        "#     columns = [description[0] for description in cursor.description]\n",
        "#     print(f\"\\nSample entry:\")\n",
        "#     for col, val in zip(columns, sample):\n",
        "#         if val and len(str(val)) > 200:\n",
        "#             print(f\"  {col}: {str(val)[:200]}...\")\n",
        "#         else:\n",
        "#             print(f\"  {col}: {val}\")\n",
        "\n",
        "# # Show column names\n",
        "# cursor.execute('PRAGMA table_info(reasoning_steps)')\n",
        "# columns_info = cursor.fetchall()\n",
        "# print(f\"\\nDatabase columns:\")\n",
        "# for col_info in columns_info:\n",
        "#     print(f\"  {col_info[1]} ({col_info[2]})\")\n",
        "\n",
        "# conn.close()\n",
        "# print(f\"\\nDatabase saved to: {db_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "be5ec5af",
      "metadata": {},
      "outputs": [],
      "source": [
        "db_path='/home/larcanio/AIMO3_v2/mvidia_reasoning_steps.db'\n",
        "conn = sqlite3.connect(str(db_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "3f2917f9",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>problem</th>\n",
              "      <th>generated_solution</th>\n",
              "      <th>generation_model</th>\n",
              "      <th>problem_type</th>\n",
              "      <th>expected_answer</th>\n",
              "      <th>problem_source</th>\n",
              "      <th>inference_mode</th>\n",
              "      <th>pass_rate_72b_tir</th>\n",
              "      <th>used_in_kaggle</th>\n",
              "      <th>created_at</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>161902</td>\n",
              "      <td>Let $f$ be the function defined by $f(x) = -2 ...</td>\n",
              "      <td>n/a</td>\n",
              "      <td>n/a</td>\n",
              "      <td>has_answer_extracted</td>\n",
              "      <td>61</td>\n",
              "      <td>MATH_training_set</td>\n",
              "      <td>n/a</td>\n",
              "      <td>n/a</td>\n",
              "      <td>0</td>\n",
              "      <td>2026-01-28 13:10:28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>161911</td>\n",
              "      <td>A block of wood has the shape of a right circu...</td>\n",
              "      <td>n/a</td>\n",
              "      <td>n/a</td>\n",
              "      <td>has_answer_extracted</td>\n",
              "      <td>53</td>\n",
              "      <td>MATH_training_set</td>\n",
              "      <td>n/a</td>\n",
              "      <td>n/a</td>\n",
              "      <td>0</td>\n",
              "      <td>2026-01-28 13:10:28</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       id                                            problem  \\\n",
              "0  161902  Let $f$ be the function defined by $f(x) = -2 ...   \n",
              "1  161911  A block of wood has the shape of a right circu...   \n",
              "\n",
              "  generated_solution generation_model          problem_type expected_answer  \\\n",
              "0                n/a              n/a  has_answer_extracted              61   \n",
              "1                n/a              n/a  has_answer_extracted              53   \n",
              "\n",
              "      problem_source inference_mode pass_rate_72b_tir used_in_kaggle  \\\n",
              "0  MATH_training_set            n/a               n/a              0   \n",
              "1  MATH_training_set            n/a               n/a              0   \n",
              "\n",
              "            created_at  \n",
              "0  2026-01-28 13:10:28  \n",
              "1  2026-01-28 13:10:28  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>problem</th>\n",
              "      <th>generated_solution</th>\n",
              "      <th>generation_model</th>\n",
              "      <th>problem_type</th>\n",
              "      <th>expected_answer</th>\n",
              "      <th>problem_source</th>\n",
              "      <th>inference_mode</th>\n",
              "      <th>pass_rate_72b_tir</th>\n",
              "      <th>used_in_kaggle</th>\n",
              "      <th>created_at</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>199998</th>\n",
              "      <td>1145940</td>\n",
              "      <td>Find the least positive integer \\( n \\) such t...</td>\n",
              "      <td>&lt;think&gt;\\nOkay, so I need to find the smallest ...</td>\n",
              "      <td>DeepSeek-R1</td>\n",
              "      <td>has_answer_extracted</td>\n",
              "      <td>4</td>\n",
              "      <td>aops_c6_high_school_olympiads</td>\n",
              "      <td>cot</td>\n",
              "      <td>0.96875</td>\n",
              "      <td>1</td>\n",
              "      <td>2026-01-28 13:12:35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199999</th>\n",
              "      <td>1145957</td>\n",
              "      <td>Students 1 through 9 are assigned cards number...</td>\n",
              "      <td>&lt;think&gt;\\nOkay, so I need to figure out how man...</td>\n",
              "      <td>DeepSeek-R1</td>\n",
              "      <td>has_answer_extracted</td>\n",
              "      <td>77760</td>\n",
              "      <td>aops_c4_high_school_math</td>\n",
              "      <td>cot</td>\n",
              "      <td>0.59375</td>\n",
              "      <td>0</td>\n",
              "      <td>2026-01-28 13:12:35</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             id                                            problem  \\\n",
              "199998  1145940  Find the least positive integer \\( n \\) such t...   \n",
              "199999  1145957  Students 1 through 9 are assigned cards number...   \n",
              "\n",
              "                                       generated_solution generation_model  \\\n",
              "199998  <think>\\nOkay, so I need to find the smallest ...      DeepSeek-R1   \n",
              "199999  <think>\\nOkay, so I need to figure out how man...      DeepSeek-R1   \n",
              "\n",
              "                problem_type expected_answer                 problem_source  \\\n",
              "199998  has_answer_extracted               4  aops_c6_high_school_olympiads   \n",
              "199999  has_answer_extracted           77760       aops_c4_high_school_math   \n",
              "\n",
              "       inference_mode pass_rate_72b_tir used_in_kaggle           created_at  \n",
              "199998            cot           0.96875              1  2026-01-28 13:12:35  \n",
              "199999            cot           0.59375              0  2026-01-28 13:12:35  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def query_database(query, connection=None):\n",
        "    if connection is None:\n",
        "        connection = conn\n",
        "    df = pd.read_sql_query(query, connection)\n",
        "    return df\n",
        "\n",
        "# Query database for problems with numeric answers\n",
        "query = \"\"\"\n",
        "SELECT *\n",
        "FROM reasoning_steps\n",
        "  WHERE expected_answer GLOB '[0-9]*'\n",
        "  AND expected_answer NOT GLOB '*[^0-9]*'\n",
        "  AND CAST(expected_answer AS INTEGER) BETWEEN 0 AND 99999\n",
        "  AND (\n",
        "        expected_answer = '0'\n",
        "        OR expected_answer NOT LIKE '0%'\n",
        "      )\n",
        "  AND problem_type = 'has_answer_extracted'\n",
        "LIMIT 200000;\n",
        "\"\"\"\n",
        "problems = query_database(query)\n",
        "print(f\"Loaded {len(problems)} problems from database\")\n",
        "\n",
        "display(problems.head(2))\n",
        "display(problems.tail(2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "domain_filter",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filtering by domains: ['number_theory']\n",
            "Problems before domain filter: 200000\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fc6a266499614e6da634a6c63499fd24",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Classifying domains:   0%|          | 0/200000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Domain distribution (before filter):\n",
            "_classified_domain\n",
            "algebra          118237\n",
            "combinatorics     34799\n",
            "geometry          27617\n",
            "number_theory     19347\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Excluded 180653 problems outside target domains\n",
            "Problems after domain filter: 19347\n"
          ]
        }
      ],
      "source": [
        "# Domain Filtering\n",
        "from domain_classifier_2nd_stage import (\n",
        "    extract_fields,\n",
        "    compute_all_scores,\n",
        "    get_heuristic_ranking,\n",
        "    get_hard_override,\n",
        "    ALLOWED_DOMAINS,\n",
        ")\n",
        "\n",
        "# Set to None to disable filtering, or specify domains to keep\n",
        "# Options: \"algebra\", \"number_theory\", \"combinatorics\", \"geometry\"\n",
        "FILTER_DOMAINS = [\"number_theory\"]\n",
        "\n",
        "\n",
        "def classify_problem_domain(problem_text: str) -> str:\n",
        "    \"\"\"Classify a problem's domain using heuristic patterns.\"\"\"\n",
        "    payload = {\n",
        "        \"problem\": {\"text\": problem_text},\n",
        "        \"code\": \"\",\n",
        "        \"plan\": \"\",\n",
        "        \"goal\": \"\",\n",
        "    }\n",
        "    \n",
        "    fields = extract_fields(payload)\n",
        "    scores = compute_all_scores(fields)\n",
        "    \n",
        "    forced = get_hard_override(fields)\n",
        "    if forced:\n",
        "        return forced\n",
        "    \n",
        "    heur_best, _, _ = get_heuristic_ranking(scores)\n",
        "    return heur_best\n",
        "\n",
        "\n",
        "if FILTER_DOMAINS:\n",
        "    invalid = set(FILTER_DOMAINS) - ALLOWED_DOMAINS\n",
        "    if invalid:\n",
        "        raise ValueError(f\"Invalid domain names: {invalid}. Allowed: {ALLOWED_DOMAINS}\")\n",
        "    \n",
        "    pre_filter_count = len(problems)\n",
        "    print(f\"Filtering by domains: {FILTER_DOMAINS}\")\n",
        "    print(f\"Problems before filter: {pre_filter_count}\")\n",
        "    \n",
        "    domain_classifications = []\n",
        "    for _, row in tqdm(problems.iterrows(), total=len(problems), desc=\"Classifying\"):\n",
        "        domain = classify_problem_domain(row['problem'])\n",
        "        domain_classifications.append(domain)\n",
        "    \n",
        "    problems['_classified_domain'] = domain_classifications\n",
        "    \n",
        "    print(\"\\nDomain distribution:\")\n",
        "    print(problems['_classified_domain'].value_counts())\n",
        "    \n",
        "    problems = problems[problems['_classified_domain'].isin(FILTER_DOMAINS)].copy()\n",
        "    problems = problems.drop(columns=['_classified_domain'])\n",
        "    \n",
        "    post_filter_count = len(problems)\n",
        "    excluded = pre_filter_count - post_filter_count\n",
        "    print(f\"Excluded {excluded} problems | Remaining: {post_filter_count}\")\n",
        "else:\n",
        "    print(\"Domain filtering disabled\")\n",
        "\n",
        "problems = problems[:5000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "8dqhzn17b9i",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Problems from database query: 5000\n",
            "Found 35570 existing problem_ids in reference file\n",
            "Excluded 1411 duplicate problems\n",
            "Final problem count: 3589\n"
          ]
        }
      ],
      "source": [
        "# Deduplication against reference dataset\n",
        "REFERENCE_JSONL = Path('/home/larcanio/AIMO3_v2/data/datasets/mvidia_reasoning_steps/Compiled-OpenMath/dataset.jsonl')\n",
        "\n",
        "initial_count = len(problems)\n",
        "print(f\"Problems from query: {initial_count}\")\n",
        "\n",
        "if REFERENCE_JSONL and REFERENCE_JSONL.exists():\n",
        "    existing_problem_ids = set()\n",
        "    with open(REFERENCE_JSONL, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                record = json.loads(line)\n",
        "                pid = record.get('problem_id') or record.get('problem', {}).get('problem_id')\n",
        "                if pid is not None:\n",
        "                    existing_problem_ids.add(str(pid))\n",
        "    \n",
        "    print(f\"Found {len(existing_problem_ids)} existing problems in reference\")\n",
        "    \n",
        "    problems = problems[~problems['id'].astype(str).isin(existing_problem_ids)]\n",
        "    filtered_count = len(problems)\n",
        "    excluded_count = initial_count - filtered_count\n",
        "    print(f\"Excluded {excluded_count} duplicates | Remaining: {filtered_count}\")\n",
        "else:\n",
        "    print(\"Reference file not found or not specified, skipping deduplication\")\n",
        "\n",
        "print(f\"Final problem count: {len(problems)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "4a476f81",
      "metadata": {},
      "outputs": [],
      "source": [
        "import multiprocessing as mp\n",
        "import traceback\n",
        "\n",
        "def _exec_worker(code: str, queue: mp.Queue):\n",
        "    buf = io.StringIO()\n",
        "    try:\n",
        "        globals_dict = {\"__name__\": \"__main__\"}\n",
        "        with contextlib.redirect_stdout(buf):\n",
        "            exec(code, globals_dict, globals_dict)\n",
        "        queue.put((\"ok\", buf.getvalue()))\n",
        "    except Exception:\n",
        "        queue.put((\"error\", traceback.format_exc()))\n",
        "\n",
        "def exec_with_timeout_capture_stdout(code: str, timeout_seconds: int = 60) -> str:\n",
        "    queue = mp.Queue()\n",
        "    p = mp.Process(target=_exec_worker, args=(code, queue), daemon=True)\n",
        "    p.start()\n",
        "    p.join(timeout_seconds)\n",
        "\n",
        "    try:\n",
        "        if p.is_alive():\n",
        "            p.terminate()\n",
        "            p.join(timeout=2)\n",
        "            if p.is_alive():\n",
        "                p.kill()\n",
        "                p.join()\n",
        "            raise TimeoutError(f\"Execution exceeded {timeout_seconds} seconds\")\n",
        "\n",
        "        if queue.empty():\n",
        "            raise RuntimeError(\"Execution finished but produced no result\")\n",
        "\n",
        "        status, payload = queue.get_nowait()\n",
        "        if status == \"error\":\n",
        "            raise RuntimeError(payload)\n",
        "\n",
        "        return payload\n",
        "    finally:\n",
        "        if p.is_alive():\n",
        "            p.kill()\n",
        "            p.join()\n",
        "        queue.close()\n",
        "        queue.join_thread()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "bba9dcab",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def cleanup_zombie_processes():\n",
        "    try:\n",
        "        while True:\n",
        "            pid, status = os.waitpid(-1, os.WNOHANG)\n",
        "            if pid == 0:\n",
        "                break\n",
        "    except ChildProcessError:\n",
        "        pass\n",
        "\n",
        "cleanup_zombie_processes()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "e4dc0eb4",
      "metadata": {},
      "outputs": [],
      "source": [
        "def strip_markdown_code_blocks(code: str) -> str:\n",
        "    if not code:\n",
        "        return code\n",
        "    code = code.strip()\n",
        "    if code.startswith('```'):\n",
        "        lines = code.split('\\n')\n",
        "        if lines[0].startswith('```'):\n",
        "            lines = lines[1:]\n",
        "        if lines and lines[-1].strip() == '```':\n",
        "            lines = lines[:-1]\n",
        "        code = '\\n'.join(lines)\n",
        "    return code.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "ef19298d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup output directory and files\n",
        "output_dir = OUTPUT_BASE_DIR\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "run_timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "run_dir = output_dir / (MODEL_NAME + \"_\" + run_timestamp)\n",
        "run_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "jsonl_file = run_dir / \"dataset.jsonl\"\n",
        "\n",
        "prompt_files = {\n",
        "    \"base\": run_dir / f\"base_prompt_{PROMPT_VERSION}.txt\",\n",
        "    \"timeout_repair\": run_dir / f\"timeout_repair_prompt_{PROMPT_VERSION}.txt\",\n",
        "    \"error_repair\": run_dir / f\"error_repair_prompt_{PROMPT_VERSION}.txt\",\n",
        "    \"wrong_answer_repair\": run_dir / f\"wrong_answer_repair_prompt_{PROMPT_VERSION}.txt\",\n",
        "}\n",
        "\n",
        "def create_datapoint_id(dataset, problem_id, timestamp, sequence):\n",
        "    \"\"\"Generate unique datapoint ID.\"\"\"\n",
        "    return f\"{dataset}_{problem_id}_{timestamp}_{sequence:03d}\"\n",
        "\n",
        "def classify_failure(error, is_timeout):\n",
        "    \"\"\"Classify failure type for tracking.\"\"\"\n",
        "    if is_timeout:\n",
        "        return {\n",
        "            \"failure_regime\": \"timeout\",\n",
        "            \"error_signature\": \"TimeoutError\",\n",
        "            \"retry_pattern\": None\n",
        "        }\n",
        "\n",
        "    if error is None:\n",
        "        return None\n",
        "\n",
        "    error_str = str(error)\n",
        "\n",
        "    if \"Traceback\" in error_str:\n",
        "        lines = error_str.split('\\n')\n",
        "        error_signature = None\n",
        "        for line in lines:\n",
        "            if 'Error:' in line or 'Exception:' in line:\n",
        "                error_signature = line.strip()\n",
        "                break\n",
        "        if error_signature is None:\n",
        "            error_signature = f\"RuntimeError: {error_str.split(chr(10))[0]}\"\n",
        "    else:\n",
        "        error_signature = f\"RuntimeError: {error_str.split(chr(10))[0][:200]}\"\n",
        "\n",
        "    return {\n",
        "        \"failure_regime\": \"runtime_error\",\n",
        "        \"error_signature\": error_signature,\n",
        "        \"retry_pattern\": None\n",
        "    }\n",
        "\n",
        "def save_datapoint(datapoint):\n",
        "    \"\"\"Append datapoint to JSONL file.\"\"\"\n",
        "    with open(jsonl_file, 'a', encoding='utf-8') as f:\n",
        "        f.write(json.dumps(datapoint, ensure_ascii=False) + '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "0e14bb6a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load prompt templates from files\n",
        "PROMPT_DIR = Path(\"prompts\")\n",
        "\n",
        "BASE_PROMPT = (PROMPT_DIR / \"nb0_code_generation_base.md\").read_text(encoding=\"utf-8\")\n",
        "TIMEOUT_REPAIR_TEMPLATE = (PROMPT_DIR / \"nb0_code_repair_timeout.md\").read_text(encoding=\"utf-8\")\n",
        "ERROR_REPAIR_TEMPLATE = (PROMPT_DIR / \"nb0_code_repair_error.md\").read_text(encoding=\"utf-8\")\n",
        "WRONG_ANSWER_REPAIR_TEMPLATE = (PROMPT_DIR / \"nb0_code_repair_wrong_answer.md\").read_text(encoding=\"utf-8\")\n",
        "VERIFIER_PROMPT_TEMPLATE = (PROMPT_DIR / \"nb0_code_verifier.md\").read_text(encoding=\"utf-8\")\n",
        "REPAIR_PROMPT_TEMPLATE = (PROMPT_DIR / \"nb0_code_repair_quality.md\").read_text(encoding=\"utf-8\")\n",
        "\n",
        "print(f\"Loaded {len([BASE_PROMPT, TIMEOUT_REPAIR_TEMPLATE, ERROR_REPAIR_TEMPLATE, WRONG_ANSWER_REPAIR_TEMPLATE, VERIFIER_PROMPT_TEMPLATE, REPAIR_PROMPT_TEMPLATE])} prompt templates from {PROMPT_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "ns38gu1mhkb",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompts saved to /home/larcanio/AIMO3_v2/data/datasets/mvidia_reasoning_steps/DeepSeek-R1/gpt-oss_2026-02-04_02-30-10\n"
          ]
        }
      ],
      "source": [
        "# Save prompts for reproducibility\n",
        "with open(prompt_files[\"base\"], 'w', encoding='utf-8') as f:\n",
        "    f.write(BASE_PROMPT)\n",
        "\n",
        "with open(prompt_files[\"timeout_repair\"], 'w', encoding='utf-8') as f:\n",
        "    f.write(TIMEOUT_REPAIR_TEMPLATE)\n",
        "\n",
        "with open(prompt_files[\"error_repair\"], 'w', encoding='utf-8') as f:\n",
        "    f.write(ERROR_REPAIR_TEMPLATE)\n",
        "\n",
        "with open(prompt_files[\"wrong_answer_repair\"], 'w', encoding='utf-8') as f:\n",
        "    f.write(WRONG_ANSWER_REPAIR_TEMPLATE)\n",
        "\n",
        "print(f\"Prompts saved to: {run_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "c8f55de8",
      "metadata": {},
      "outputs": [],
      "source": [
        "async def process_problem(pool: LLMPool, cfg, row, idx, sequence_num):\n",
        "    \"\"\"Process a single problem with two-phase retry matching the original schema.\n",
        "\n",
        "    Phase 1 – Execution retries (up to MAX_EXECUTION_RETRIES):\n",
        "      - TimeoutError  -> TIMEOUT_REPAIR_TEMPLATE\n",
        "      - RuntimeError  -> ERROR_REPAIR_TEMPLATE\n",
        "\n",
        "    Phase 2 – Answer retries (up to MAX_ANSWER_RETRIES):\n",
        "      - Wrong answer  -> WRONG_ANSWER_REPAIR_TEMPLATE\n",
        "\n",
        "    Returns:\n",
        "        (datapoint, found_correct, tok_in_total, tok_out_total)\n",
        "    \"\"\"\n",
        "    problem_text = row[\"problem\"]\n",
        "    solution_text = row[\"generated_solution\"]\n",
        "    expected_answer = str(row[\"expected_answer\"])\n",
        "    problem_id = str(row.get(\"id\", idx))\n",
        "\n",
        "    base_prompt = BASE_PROMPT.format(problem=problem_text, solution=solution_text)\n",
        "\n",
        "    timestamp = datetime.now().isoformat()\n",
        "    datapoint_id = create_datapoint_id(\n",
        "        \"mvidia_reasoning_steps\", problem_id,\n",
        "        datetime.now().strftime(\"%Y%m%d\"), sequence_num,\n",
        "    )\n",
        "\n",
        "    datapoint = {\n",
        "        \"id\": datapoint_id,\n",
        "        \"dataset\": \"mvidia_reasoning_steps\",\n",
        "        \"problem_id\": problem_id,\n",
        "        \"timestamp\": timestamp,\n",
        "        \"problem\": {\n",
        "            \"text\": problem_text,\n",
        "            \"expected_answer\": expected_answer,\n",
        "            \"problem_type\": row.get(\"problem_type\", \"\"),\n",
        "            \"problem_source\": row.get(\"problem_source\", \"\"),\n",
        "            \"original_solution\": row.get(\"generated_solution\", \"\"),\n",
        "        },\n",
        "        \"prompt_ids\": {\n",
        "            \"base\": f\"base_prompt_{PROMPT_VERSION}\",\n",
        "            \"timeout_repair\": f\"timeout_repair_prompt_{PROMPT_VERSION}\",\n",
        "            \"error_repair\": f\"error_repair_prompt_{PROMPT_VERSION}\",\n",
        "            \"wrong_answer_repair\": f\"wrong_answer_repair_prompt_{PROMPT_VERSION}\",\n",
        "            \"reasoning_summary\": None,\n",
        "        },\n",
        "        \"models\": {\n",
        "            \"generating\": MODEL_NAME,\n",
        "            \"fixing\": MODEL_NAME,\n",
        "        },\n",
        "        \"generation_config\": {\n",
        "            \"temperature\": TEMPERATURE,\n",
        "            \"max_tokens\": MAX_TOKENS,\n",
        "            \"reasoning_effort\": REASONING_EFFORT,\n",
        "        },\n",
        "        \"reasoning_summary_config\": {\n",
        "            \"enabled\": False,\n",
        "            \"reasoning_effort\": None,\n",
        "            \"max_tokens\": None,\n",
        "        },\n",
        "        \"attempts\": [],\n",
        "    }\n",
        "\n",
        "    tok_in_total = 0\n",
        "    tok_out_total = 0\n",
        "\n",
        "    # ── Phase 1: Execution retry loop ────────────────────────────────────\n",
        "    response = None\n",
        "    captured_stdout = None\n",
        "    last_error = None\n",
        "    is_timeout = False\n",
        "    execution_success = False\n",
        "\n",
        "    for attempt in range(cfg.MAX_EXECUTION_RETRIES):\n",
        "        attempt_timestamp = datetime.now().isoformat()\n",
        "\n",
        "        if attempt == 0:\n",
        "            user_message = base_prompt\n",
        "            stage = \"initial\"\n",
        "        elif is_timeout:\n",
        "            user_message = TIMEOUT_REPAIR_TEMPLATE.format(\n",
        "                timeout_seconds=cfg.EXECUTION_TIMEOUT_SECONDS,\n",
        "                problem=problem_text,\n",
        "                solution=solution_text,\n",
        "                previous_code=response,\n",
        "                error=str(last_error),\n",
        "                base_prompt=base_prompt,\n",
        "            )\n",
        "            stage = \"execution_repair\"\n",
        "        else:\n",
        "            user_message = ERROR_REPAIR_TEMPLATE.format(\n",
        "                problem=problem_text,\n",
        "                solution=solution_text,\n",
        "                previous_code=response,\n",
        "                error=str(last_error),\n",
        "                base_prompt=base_prompt,\n",
        "            )\n",
        "            stage = \"execution_repair\"\n",
        "\n",
        "        attempt_record = {\n",
        "            \"n\": attempt + 1,\n",
        "            \"stage\": stage,\n",
        "            \"model\": MODEL_NAME,\n",
        "            \"timestamp\": attempt_timestamp,\n",
        "            \"code\": None,\n",
        "            \"exec\": {\n",
        "                \"status\": None,\n",
        "                \"stdout\": None,\n",
        "                \"stderr\": None,\n",
        "                \"error\": None,\n",
        "                \"error_type\": None,\n",
        "                \"timeout\": False,\n",
        "                \"duration\": None,\n",
        "            },\n",
        "            \"result\": {\n",
        "                \"predicted\": None,\n",
        "                \"correct\": None,\n",
        "            },\n",
        "            \"retry_reason\": None,\n",
        "        }\n",
        "\n",
        "        # ── LLM call ─────────────────────────────────────────────────────\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": SYSTEM_MESSAGE},\n",
        "            {\"role\": \"user\", \"content\": user_message},\n",
        "        ]\n",
        "\n",
        "        try:\n",
        "            resp = await pool.request(\n",
        "                messages,\n",
        "                temperature=TEMPERATURE,\n",
        "                max_tokens=MAX_TOKENS,\n",
        "                reasoning_effort=REASONING_EFFORT,\n",
        "            )\n",
        "        except Exception as e:\n",
        "            status_code = getattr(e, \"status_code\", None)\n",
        "            err_detail = getattr(e, \"message\", str(e))\n",
        "            print(\n",
        "                f\"LLM error (problem {idx}, attempt {attempt + 1}): \"\n",
        "                f\"{type(e).__name__}\"\n",
        "                f\"{f' [{status_code}]' if status_code else ''}\"\n",
        "                f\" - {str(err_detail)[:300]}\"\n",
        "            )\n",
        "            continue\n",
        "\n",
        "        response = resp.content\n",
        "        if not response:\n",
        "            continue\n",
        "\n",
        "        response = strip_markdown_code_blocks(response)\n",
        "        attempt_record[\"code\"] = response\n",
        "        tok_in_total += resp.prompt_tokens\n",
        "        tok_out_total += resp.completion_tokens\n",
        "\n",
        "        # ── Execute generated code ────────────────────────────────────────\n",
        "        exec_start = time.monotonic()\n",
        "        try:\n",
        "            captured_stdout = await anyio.to_thread.run_sync(\n",
        "                lambda c=response, t=cfg.EXECUTION_TIMEOUT_SECONDS: (\n",
        "                    exec_with_timeout_capture_stdout(c, t)\n",
        "                )\n",
        "            )\n",
        "            exec_duration = time.monotonic() - exec_start\n",
        "\n",
        "            attempt_record[\"exec\"][\"status\"] = \"success\"\n",
        "            attempt_record[\"exec\"][\"stdout\"] = captured_stdout\n",
        "            attempt_record[\"exec\"][\"duration\"] = exec_duration\n",
        "            execution_success = True\n",
        "            datapoint[\"attempts\"].append(attempt_record)\n",
        "            break\n",
        "\n",
        "        except TimeoutError as e:\n",
        "            exec_duration = time.monotonic() - exec_start\n",
        "            last_error = e\n",
        "            is_timeout = True\n",
        "\n",
        "            attempt_record[\"exec\"][\"status\"] = \"timeout\"\n",
        "            attempt_record[\"exec\"][\"error\"] = str(e)\n",
        "            attempt_record[\"exec\"][\"error_type\"] = \"TimeoutError\"\n",
        "            attempt_record[\"exec\"][\"timeout\"] = True\n",
        "            attempt_record[\"exec\"][\"duration\"] = exec_duration\n",
        "            attempt_record[\"retry_reason\"] = \"timeout\"\n",
        "\n",
        "        except Exception as e:\n",
        "            exec_duration = time.monotonic() - exec_start\n",
        "            last_error = e\n",
        "            is_timeout = False\n",
        "\n",
        "            attempt_record[\"exec\"][\"status\"] = \"error\"\n",
        "            attempt_record[\"exec\"][\"error\"] = str(e)\n",
        "            attempt_record[\"exec\"][\"error_type\"] = type(e).__name__\n",
        "            attempt_record[\"exec\"][\"duration\"] = exec_duration\n",
        "            attempt_record[\"retry_reason\"] = \"runtime_error\"\n",
        "\n",
        "        datapoint[\"attempts\"].append(attempt_record)\n",
        "\n",
        "    # ── Phase 2: Answer retry loop ───────────────────────────────────────\n",
        "    if execution_success and captured_stdout is not None:\n",
        "        predicted = captured_stdout.strip()\n",
        "        expected = expected_answer.strip()\n",
        "\n",
        "        # Set result on the last execution attempt\n",
        "        if datapoint[\"attempts\"]:\n",
        "            last_att = datapoint[\"attempts\"][-1]\n",
        "            last_att[\"result\"][\"predicted\"] = predicted\n",
        "            last_att[\"result\"][\"correct\"] = (predicted == expected)\n",
        "\n",
        "        is_correct = (predicted == expected)\n",
        "        answer_retry_count = 0\n",
        "\n",
        "        while answer_retry_count < cfg.MAX_ANSWER_RETRIES and not is_correct:\n",
        "            answer_retry_count += 1\n",
        "            answer_retry_timestamp = datetime.now().isoformat()\n",
        "\n",
        "            answer_attempt_record = {\n",
        "                \"n\": len(datapoint[\"attempts\"]) + 1,\n",
        "                \"stage\": \"answer_repair\",\n",
        "                \"model\": MODEL_NAME,\n",
        "                \"timestamp\": answer_retry_timestamp,\n",
        "                \"code\": None,\n",
        "                \"exec\": {\n",
        "                    \"status\": None,\n",
        "                    \"stdout\": None,\n",
        "                    \"stderr\": None,\n",
        "                    \"error\": None,\n",
        "                    \"error_type\": None,\n",
        "                    \"timeout\": False,\n",
        "                    \"duration\": None,\n",
        "                },\n",
        "                \"result\": {\n",
        "                    \"predicted\": None,\n",
        "                    \"correct\": None,\n",
        "                },\n",
        "                \"retry_reason\": \"wrong_answer\",\n",
        "            }\n",
        "\n",
        "            user_message = WRONG_ANSWER_REPAIR_TEMPLATE.format(\n",
        "                problem=problem_text,\n",
        "                solution=solution_text,\n",
        "                previous_code=response,\n",
        "                wrong_answer=predicted,\n",
        "                expected_answer=expected,\n",
        "                base_prompt=base_prompt,\n",
        "            )\n",
        "\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": SYSTEM_MESSAGE},\n",
        "                {\"role\": \"user\", \"content\": user_message},\n",
        "            ]\n",
        "\n",
        "            try:\n",
        "                resp = await pool.request(\n",
        "                    messages,\n",
        "                    temperature=TEMPERATURE,\n",
        "                    max_tokens=MAX_TOKENS,\n",
        "                    reasoning_effort=REASONING_EFFORT,\n",
        "                )\n",
        "            except Exception as e:\n",
        "                status_code = getattr(e, \"status_code\", None)\n",
        "                err_detail = getattr(e, \"message\", str(e))\n",
        "                print(\n",
        "                    f\"LLM error (answer repair, problem {idx}): \"\n",
        "                    f\"{type(e).__name__}\"\n",
        "                    f\"{f' [{status_code}]' if status_code else ''}\"\n",
        "                    f\" - {str(err_detail)[:300]}\"\n",
        "                )\n",
        "                break\n",
        "\n",
        "            response = resp.content\n",
        "            if not response:\n",
        "                break\n",
        "\n",
        "            response = strip_markdown_code_blocks(response)\n",
        "            answer_attempt_record[\"code\"] = response\n",
        "            tok_in_total += resp.prompt_tokens\n",
        "            tok_out_total += resp.completion_tokens\n",
        "\n",
        "            exec_start = time.monotonic()\n",
        "            try:\n",
        "                captured_stdout = await anyio.to_thread.run_sync(\n",
        "                    lambda c=response, t=cfg.EXECUTION_TIMEOUT_SECONDS: (\n",
        "                        exec_with_timeout_capture_stdout(c, t)\n",
        "                    )\n",
        "                )\n",
        "                exec_duration = time.monotonic() - exec_start\n",
        "\n",
        "                predicted = captured_stdout.strip()\n",
        "                answer_attempt_record[\"exec\"][\"status\"] = \"success\"\n",
        "                answer_attempt_record[\"exec\"][\"stdout\"] = captured_stdout\n",
        "                answer_attempt_record[\"exec\"][\"duration\"] = exec_duration\n",
        "                answer_attempt_record[\"result\"][\"predicted\"] = predicted\n",
        "                answer_attempt_record[\"result\"][\"correct\"] = (predicted == expected)\n",
        "                is_correct = (predicted == expected)\n",
        "                datapoint[\"attempts\"].append(answer_attempt_record)\n",
        "\n",
        "            except TimeoutError as e:\n",
        "                exec_duration = time.monotonic() - exec_start\n",
        "                answer_attempt_record[\"exec\"][\"status\"] = \"timeout\"\n",
        "                answer_attempt_record[\"exec\"][\"error\"] = str(e)\n",
        "                answer_attempt_record[\"exec\"][\"error_type\"] = \"TimeoutError\"\n",
        "                answer_attempt_record[\"exec\"][\"timeout\"] = True\n",
        "                answer_attempt_record[\"exec\"][\"duration\"] = exec_duration\n",
        "                datapoint[\"attempts\"].append(answer_attempt_record)\n",
        "                break\n",
        "\n",
        "            except Exception as e:\n",
        "                exec_duration = time.monotonic() - exec_start\n",
        "                answer_attempt_record[\"exec\"][\"status\"] = \"error\"\n",
        "                answer_attempt_record[\"exec\"][\"error\"] = str(e)\n",
        "                answer_attempt_record[\"exec\"][\"error_type\"] = type(e).__name__\n",
        "                answer_attempt_record[\"exec\"][\"duration\"] = exec_duration\n",
        "                datapoint[\"attempts\"].append(answer_attempt_record)\n",
        "                break\n",
        "\n",
        "    # ── Build outcome ────────────────────────────────────────────────────\n",
        "    if execution_success and captured_stdout is not None:\n",
        "        predicted = captured_stdout.strip()\n",
        "        expected = expected_answer.strip()\n",
        "        is_correct = (predicted == expected)\n",
        "\n",
        "        pass_at_k = None\n",
        "        for i, att in enumerate(datapoint[\"attempts\"]):\n",
        "            if att[\"exec\"][\"status\"] == \"success\" and att[\"result\"].get(\"correct\"):\n",
        "                pass_at_k = i + 1\n",
        "                break\n",
        "\n",
        "        datapoint[\"outcome\"] = {\n",
        "            \"status\": \"success\" if is_correct else \"wrong_answer\",\n",
        "            \"answer\": predicted if is_correct else None,\n",
        "            \"total_attempts\": len(datapoint[\"attempts\"]),\n",
        "            \"execution_attempts\": sum(\n",
        "                1 for a in datapoint[\"attempts\"]\n",
        "                if a[\"stage\"] in (\"initial\", \"execution_repair\")\n",
        "            ),\n",
        "            \"answer_retry_attempts\": sum(\n",
        "                1 for a in datapoint[\"attempts\"]\n",
        "                if a[\"stage\"] == \"answer_repair\"\n",
        "            ),\n",
        "            \"pass_at_k\": pass_at_k,\n",
        "        }\n",
        "\n",
        "        if not is_correct:\n",
        "            failure_info = classify_failure(None, False)\n",
        "            if failure_info:\n",
        "                datapoint[\"outcome\"][\"failure_analysis\"] = failure_info\n",
        "    else:\n",
        "        datapoint[\"outcome\"] = {\n",
        "            \"status\": \"failed\",\n",
        "            \"answer\": None,\n",
        "            \"total_attempts\": len(datapoint[\"attempts\"]),\n",
        "            \"execution_attempts\": len(datapoint[\"attempts\"]),\n",
        "            \"answer_retry_attempts\": 0,\n",
        "            \"pass_at_k\": None,\n",
        "        }\n",
        "\n",
        "        if datapoint[\"attempts\"]:\n",
        "            last_att = datapoint[\"attempts\"][-1]\n",
        "            failure_info = classify_failure(\n",
        "                last_att[\"exec\"][\"error\"] if last_att[\"exec\"][\"error\"] else None,\n",
        "                last_att[\"exec\"][\"timeout\"],\n",
        "            )\n",
        "            if failure_info:\n",
        "                datapoint[\"outcome\"][\"failure_analysis\"] = failure_info\n",
        "\n",
        "    found_correct = datapoint[\"outcome\"][\"status\"] == \"success\"\n",
        "    return datapoint, found_correct, tok_in_total, tok_out_total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "0z85jyf7u6c",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "26e0f7ef6c814cc1830094218958eea4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing:   0%|          | 0/3589 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<string>:19: SyntaxWarning: \"is\" with 'int' literal. Did you mean \"==\"?\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LLM error (problem 796, attempt 2): LLMRequestError [400] - LLM request failed [400]: {\"error\":{\"message\":\"max_tokens must be at least 1, got -148. (parameter=max_tokens, value=-148)\",\"type\":\"BadRequestError\",\"param\":\"max_tokens\",\"code\":400}}\n",
            "LLM error (answer repair, problem 998): LLMRequestError [400] - LLM request failed [400]: {\"error\":{\"message\":\"max_tokens must be at least 1, got -1461. (parameter=max_tokens, value=-1461)\",\"type\":\"BadRequestError\",\"param\":\"max_tokens\",\"code\":400}}\n",
            "LLM error (problem 1136, attempt 2): LLMRequestError [400] - LLM request failed [400]: {\"error\":{\"message\":\"max_tokens must be at least 1, got -245. (parameter=max_tokens, value=-245)\",\"type\":\"BadRequestError\",\"param\":\"max_tokens\",\"code\":400}}\n",
            "LLM error (answer repair, problem 1192): LLMRequestError [400] - LLM request failed [400]: {\"error\":{\"message\":\"max_tokens must be at least 1, got -4653. (parameter=max_tokens, value=-4653)\",\"type\":\"BadRequestError\",\"param\":\"max_tokens\",\"code\":400}}\n",
            "LLM error (answer repair, problem 1219): LLMRequestError [400] - LLM request failed [400]: {\"error\":{\"message\":\"max_tokens must be at least 1, got -3099. (parameter=max_tokens, value=-3099)\",\"type\":\"BadRequestError\",\"param\":\"max_tokens\",\"code\":400}}\n",
            "LLM error (answer repair, problem 1514): LLMRequestError [400] - LLM request failed [400]: {\"error\":{\"message\":\"max_tokens must be at least 1, got -3148. (parameter=max_tokens, value=-3148)\",\"type\":\"BadRequestError\",\"param\":\"max_tokens\",\"code\":400}}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrying llm_pool.LLMPool._post_with_retry.<locals>._do_post in 2.77402622738568 seconds as it raised ReadTimeout: .\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LLM error (answer repair, problem 1593): LLMRequestError [400] - LLM request failed [400]: {\"error\":{\"message\":\"max_tokens must be at least 1, got -42. (parameter=max_tokens, value=-42)\",\"type\":\"BadRequestError\",\"param\":\"max_tokens\",\"code\":400}}\n",
            "LLM error (answer repair, problem 1994): LLMRequestError [400] - LLM request failed [400]: {\"error\":{\"message\":\"max_tokens must be at least 1, got -2856. (parameter=max_tokens, value=-2856)\",\"type\":\"BadRequestError\",\"param\":\"max_tokens\",\"code\":400}}\n",
            "LLM error (answer repair, problem 2018): LLMRequestError [400] - LLM request failed [400]: {\"error\":{\"message\":\"max_tokens must be at least 1, got -2807. (parameter=max_tokens, value=-2807)\",\"type\":\"BadRequestError\",\"param\":\"max_tokens\",\"code\":400}}\n",
            "LLM error (answer repair, problem 2129): LLMRequestError [400] - LLM request failed [400]: {\"error\":{\"message\":\"max_tokens must be at least 1, got -12. (parameter=max_tokens, value=-12)\",\"type\":\"BadRequestError\",\"param\":\"max_tokens\",\"code\":400}}\n",
            "LLM error (answer repair, problem 2228): LLMRequestError [400] - LLM request failed [400]: {\"error\":{\"message\":\"max_tokens must be at least 1, got -1571. (parameter=max_tokens, value=-1571)\",\"type\":\"BadRequestError\",\"param\":\"max_tokens\",\"code\":400}}\n",
            "LLM error (problem 2356, attempt 2): LLMRequestError [400] - LLM request failed [400]: {\"error\":{\"message\":\"max_tokens must be at least 1, got -1976. (parameter=max_tokens, value=-1976)\",\"type\":\"BadRequestError\",\"param\":\"max_tokens\",\"code\":400}}\n",
            "LLM error (answer repair, problem 2482): LLMRequestError [400] - LLM request failed [400]: {\"error\":{\"message\":\"max_tokens must be at least 1, got -1376. (parameter=max_tokens, value=-1376)\",\"type\":\"BadRequestError\",\"param\":\"max_tokens\",\"code\":400}}\n",
            "LLM error (answer repair, problem 2942): LLMRequestError [400] - LLM request failed [400]: {\"error\":{\"message\":\"max_tokens must be at least 1, got -5110. (parameter=max_tokens, value=-5110)\",\"type\":\"BadRequestError\",\"param\":\"max_tokens\",\"code\":400}}\n",
            "LLM error (answer repair, problem 3113): LLMRequestError [400] - LLM request failed [400]: {\"error\":{\"message\":\"max_tokens must be at least 1, got -1407. (parameter=max_tokens, value=-1407)\",\"type\":\"BadRequestError\",\"param\":\"max_tokens\",\"code\":400}}\n",
            "LLM error (problem 3244, attempt 2): LLMRequestError [400] - LLM request failed [400]: {\"error\":{\"message\":\"max_tokens must be at least 1, got -4081. (parameter=max_tokens, value=-4081)\",\"type\":\"BadRequestError\",\"param\":\"max_tokens\",\"code\":400}}\n",
            "LLM error (answer repair, problem 3377): LLMRequestError [400] - LLM request failed [400]: {\"error\":{\"message\":\"max_tokens must be at least 1, got -3282. (parameter=max_tokens, value=-3282)\",\"type\":\"BadRequestError\",\"param\":\"max_tokens\",\"code\":400}}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrying llm_pool.LLMPool._post_with_retry.<locals>._do_post in 5.81218776113709 seconds as it raised ReadTimeout: .\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LLM error (problem 796, attempt 3): LLMRequestError [400] - LLM request failed [400]: {\"error\":{\"message\":\"max_tokens must be at least 1, got -148. (parameter=max_tokens, value=-148)\",\"type\":\"BadRequestError\",\"param\":\"max_tokens\",\"code\":400}}\n",
            "LLM error (problem 1136, attempt 3): LLMRequestError [400] - LLM request failed [400]: {\"error\":{\"message\":\"max_tokens must be at least 1, got -245. (parameter=max_tokens, value=-245)\",\"type\":\"BadRequestError\",\"param\":\"max_tokens\",\"code\":400}}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrying llm_pool.LLMPool._post_with_retry.<locals>._do_post in 3.5321113634044483 seconds as it raised ReadTimeout: .\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LLM error (problem 2356, attempt 3): LLMRequestError [400] - LLM request failed [400]: {\"error\":{\"message\":\"max_tokens must be at least 1, got -1976. (parameter=max_tokens, value=-1976)\",\"type\":\"BadRequestError\",\"param\":\"max_tokens\",\"code\":400}}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrying llm_pool.LLMPool._post_with_retry.<locals>._do_post in 4.6323812877803014 seconds as it raised ReadTimeout: .\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LLM error (problem 3244, attempt 3): LLMRequestError [400] - LLM request failed [400]: {\"error\":{\"message\":\"max_tokens must be at least 1, got -4081. (parameter=max_tokens, value=-4081)\",\"type\":\"BadRequestError\",\"param\":\"max_tokens\",\"code\":400}}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrying llm_pool.LLMPool._post_with_retry.<locals>._do_post in 2.301634895075943 seconds as it raised ReadTimeout: .\n",
            "Retrying llm_pool.LLMPool._post_with_retry.<locals>._do_post in 2.594515806157423 seconds as it raised ReadTimeout: .\n",
            "Retrying llm_pool.LLMPool._post_with_retry.<locals>._do_post in 1.0520680929379347 seconds as it raised ReadTimeout: .\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Complete: 2927/3589 passed, 81.55% pass rate\n",
            "Total tokens: in=29516344, out=4032009, total=33548353\n"
          ]
        }
      ],
      "source": [
        "async def run_all():\n",
        "    passed_count = 0\n",
        "    total_tok_in = 0\n",
        "    total_tok_out = 0\n",
        "\n",
        "    max_problems = len(problems) if MAX_PROBLEMS_TO_PROCESS is None else min(MAX_PROBLEMS_TO_PROCESS, len(problems))\n",
        "    write_lock = anyio.Lock()\n",
        "    counter = {\"done\": 0}\n",
        "    pbar = tqdm(total=max_problems, desc=\"Processing\")\n",
        "\n",
        "    async def config_reloader(pool):\n",
        "        \"\"\"Re-read config.json every 60s and apply changes to the pool.\"\"\"\n",
        "        while True:\n",
        "            await anyio.sleep(60)\n",
        "            cfg.reload()\n",
        "            pool._limiter.total_tokens = cfg.MAX_CONCURRENT_REQUESTS\n",
        "            pool.max_retries = cfg.LLM_REQUEST_RETRY_COUNT\n",
        "            pool._client.timeout = httpx.Timeout(cfg.LLM_REQUEST_TIMEOUT_SECONDS, connect=30)\n",
        "\n",
        "    async def process_one(pool: LLMPool, idx: int):\n",
        "        nonlocal passed_count, total_tok_in, total_tok_out\n",
        "        row = problems.iloc[idx]\n",
        "        t0 = time.monotonic()\n",
        "\n",
        "        try:\n",
        "            datapoint, found_correct, tok_in, tok_out = await process_problem(\n",
        "                pool, cfg, row, idx, idx + 1,\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"Problem {idx} failed unexpectedly: {e}\")\n",
        "            return\n",
        "\n",
        "        elapsed = time.monotonic() - t0\n",
        "\n",
        "        async with write_lock:\n",
        "            save_datapoint(datapoint)\n",
        "            total_tok_in += tok_in\n",
        "            total_tok_out += tok_out\n",
        "            if found_correct:\n",
        "                passed_count += 1\n",
        "            counter[\"done\"] += 1\n",
        "            done = counter[\"done\"]\n",
        "            pbar.update(1)\n",
        "            passrate = passed_count / done\n",
        "            pbar.set_postfix(pass_rate=f\"{passrate:.2%}\", finished=done)\n",
        "            status = \"Pass\" if found_correct else \"Fail\"\n",
        "            pass_at_k = datapoint[\"outcome\"][\"pass_at_k\"]\n",
        "            # print(\n",
        "            #     f\"{done}/{max_problems} -> {status}\"\n",
        "            #     f\" -> pass@k={pass_at_k},\"\n",
        "            #     f\" req={elapsed:.1f}s,\"\n",
        "            #     f\" tokens: in={tok_in}, out={tok_out}, total={tok_in+tok_out}\"\n",
        "            # )\n",
        "\n",
        "    async with LLMPool(\n",
        "        base_url=NGINX_BALANCER_URL,\n",
        "        api_key=API_KEY,\n",
        "        model=MODEL_NAME,\n",
        "        max_inflight=cfg.MAX_CONCURRENT_REQUESTS,\n",
        "        timeout=cfg.LLM_REQUEST_TIMEOUT_SECONDS,\n",
        "        max_retries=cfg.LLM_REQUEST_RETRY_COUNT,\n",
        "    ) as pool:\n",
        "        async with anyio.create_task_group() as tg:\n",
        "            tg.start_soon(config_reloader, pool)\n",
        "\n",
        "            async with anyio.create_task_group() as work_tg:\n",
        "                for idx in range(max_problems):\n",
        "                    work_tg.start_soon(process_one, pool, idx)\n",
        "\n",
        "            # All work finished — cancel the reloader\n",
        "            tg.cancel_scope.cancel()\n",
        "\n",
        "    pbar.close()\n",
        "    print(f\"\\nComplete: {passed_count}/{max_problems} passed, {passed_count/max_problems:.2%} pass rate\")\n",
        "    print(f\"Total tokens: in={total_tok_in}, out={total_tok_out}, total={total_tok_in+total_tok_out}\")\n",
        "\n",
        "await run_all()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
