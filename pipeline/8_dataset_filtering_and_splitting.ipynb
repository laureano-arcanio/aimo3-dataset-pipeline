{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Split: Train / Test (Normalized Format)\n",
    "\n",
    "Load normalized JSONL datasets with flexible filtering:\n",
    "- Domain-specific distributions\n",
    "- Math difficulty level range\n",
    "- Code score range  \n",
    "- Token length range\n",
    "- Target dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from typing import Optional\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Configuration ──────────────────────────────────────────────────────────\n",
    "\n",
    "# Source JSONL files to load and concatenate\n",
    "DATASET_PATHS = [\n",
    "    \"/home/larcanio/AIMO3_v2/data/datasets/Dataset_Full/bucketed/dataset_1_5_complete_effective_difficulty.jsonl\",\n",
    "]\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = \"/home/larcanio/AIMO3_v2/data/datasets/splits/algebra_specialist\"\n",
    "\n",
    "# ── Filtering Options ──────────────────────────────────────────────────────\n",
    "\n",
    "# Only include records where outcome.status == \"success\"\n",
    "REQUIRE_CORRECT = True\n",
    "\n",
    "# Effective difficulty levels to include (None = all levels)\n",
    "# Derived from computation_buckets: min level where passes >= 1\n",
    "# Available levels: 0, 1, 2, 3, 4 (None = problem unsolved by any bucket)\n",
    "EFFECTIVE_DIFFICULTY: Optional[list[int]] = [1,2,3]\n",
    "\n",
    "# Code scores to include (None = all scores)\n",
    "CODE_SCORES: Optional[list[int]] = None\n",
    "\n",
    "# Token lengths to include (None = all lengths)\n",
    "# Use a list of values, or a range like list(range(50, 1001))\n",
    "TOKEN_LENGTHS: Optional[list[int]] = list(range(0, 512))\n",
    "\n",
    "# Solution token range: (min, max) estimated tokens for problem.original_solution\n",
    "# Estimated as len(text) // 4. Set to None to disable.\n",
    "SOLUTION_TOKEN_RANGE: Optional[tuple[int, int]] = None #(0,512) #None  # e.g. (0, 500)\n",
    "\n",
    "# ── Math Structure Filters ─────────────────────────────────────────────────\n",
    "# Filters based on math_structure.from_text and math_structure.from_solution\n",
    "# Set to None to disable a filter.\n",
    "\n",
    "# Max number of constraints in math_structure.from_text.constraints (None = no limit)\n",
    "MAX_CONSTRAINT_COUNT: Optional[int] = 3\n",
    "\n",
    "# Max number of objects in math_structure.from_text.objects (None = no limit)\n",
    "MAX_OBJECT_COUNT: Optional[int] = 3\n",
    "\n",
    "# Allowed reasoning depths from math_structure.from_solution.reasoning_depth (None = all)\n",
    "REASONING_DEPTH: Optional[list[str]] = [\"shallow\", \"medium\"] # [\"deep\", \"shallow\", \"medium\"]\n",
    "\n",
    "# ── Domain Distribution ────────────────────────────────────────────────────\n",
    "# Specify percentage (1-100) of final dataset per domain.\n",
    "# Percentages should sum to 100. Set to None to use all domains equally.\n",
    "\n",
    "DOMAIN_DISTRIBUTION: Optional[dict[str, int]] = {\n",
    "    \"algebra\": 90,\n",
    "    \"combinatorics\": 5,\n",
    "    \"geometry\": 2,\n",
    "    \"number_theory\": 1,\n",
    "    # Domains not listed will be excluded unless INCLUDE_UNLISTED_DOMAINS = True\n",
    "}\n",
    "\n",
    "# ── Dataset Source Distribution ────────────────────────────────────────────\n",
    "# Specify percentage (1-100) of final dataset per source dataset (record[\"dataset\"]).\n",
    "# Percentages should sum to <= 100. Set to None to keep all records (no resampling).\n",
    "# Applied after domain distribution.\n",
    "# Available datasets: gsm8k, numina1.5, mvidia_reasoning_steps,\n",
    "#   numina1.5:aops, numina1.5:cn_contest, numina1.5:metamath,\n",
    "#   numina1.5:inequalities, numina1.5:number_theory\n",
    "\n",
    "DATASET_DISTRIBUTION: Optional[dict[str, int]] = None\n",
    "\n",
    "# \tdataset\ttotal\ttrain\ttest\tpct_of_total\n",
    "# 0\tgsm8k\n",
    "# 1\tnumina1.5\n",
    "# 2\tmvidia_reasoning_steps\n",
    "# 3\tnumina1.5:metamath\n",
    "# 4\tnumina1.5:cn_contest\n",
    "# 5\tnumina1.5:aops\n",
    "# 6\tnumina1.5:inequalities\n",
    "\n",
    "# Example:\n",
    "DATASET_DISTRIBUTION: Optional[dict[str, int]] = {\n",
    "    \"gsm8k\": 80,\n",
    "    \"numina1.5\": 15,\n",
    "    \"mvidia_reasoning_steps\": 5,\n",
    "}\n",
    "\n",
    "# ── Target Dataset Size ────────────────────────────────────────────────────\n",
    "# Maximum total records before split. Set to None for no limit.\n",
    "TARGET_TOTAL_RECORDS: Optional[int] = 20000\n",
    "\n",
    "# ── Train/Test Split ───────────────────────────────────────────────────────\n",
    "TRAIN_RATIO = 0.9\n",
    "TEST_RATIO = 0.1\n",
    "RANDOM_SEED = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 71,832 records from bucketed/dataset_1_5_complete_effective_difficulty.jsonl\n",
      "\n",
      "Total records loaded: 71,832\n"
     ]
    }
   ],
   "source": [
    "# ── Load & concatenate ─────────────────────────────────────────────────────\n",
    "\n",
    "records = []\n",
    "for path in DATASET_PATHS:\n",
    "    count = 0\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                records.append(json.loads(line))\n",
    "                count += 1\n",
    "    print(f\"Loaded {count:,} records from {Path(path).parent.name}/{Path(path).name}\")\n",
    "\n",
    "print(f\"\\nTotal records loaded: {len(records):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filtering: 5,997 -> 5,997  (dropped 0)\n",
      "\n",
      "Active filters:\n",
      "  - Require correct: True\n",
      "  - Effective difficulty: [0, 1, 2, 3]\n",
      "  - Code scores: all\n",
      "  - Token lengths: [0, 511] (512 values)\n",
      "  - Solution tokens: all\n",
      "  - Max constraint count: 3\n",
      "  - Max object count: 3\n",
      "  - Reasoning depth: ['shallow', 'medium']\n"
     ]
    }
   ],
   "source": [
    "# ── Apply filters ──────────────────────────────────────────────────────────\n",
    "\n",
    "# Convert lists to sets for O(1) lookup\n",
    "_effective_difficulty_set = set(EFFECTIVE_DIFFICULTY) if EFFECTIVE_DIFFICULTY else None\n",
    "_code_scores_set = set(CODE_SCORES) if CODE_SCORES else None\n",
    "_token_lengths_set = set(TOKEN_LENGTHS) if TOKEN_LENGTHS else None\n",
    "_reasoning_depth_set = set(REASONING_DEPTH) if REASONING_DEPTH else None\n",
    "\n",
    "\n",
    "def get_effective_difficulty(r: dict) -> int | None:\n",
    "    \"\"\"Compute effective difficulty: minimum bucket level where passes >= 1.\"\"\"\n",
    "    buckets = r.get(\"computation_buckets\", [])\n",
    "    passing = [b[\"level\"] for b in buckets if b.get(\"passes\", 0) >= 1]\n",
    "    return min(passing) if passing else None\n",
    "\n",
    "\n",
    "def _estimate_tokens(text: str) -> int:\n",
    "    \"\"\"Estimate token count from text (avg ~4 chars per token).\"\"\"\n",
    "    return len(text) // 4\n",
    "\n",
    "\n",
    "def passes_filters(r: dict) -> bool:\n",
    "    \"\"\"Check if a record passes all configured filters.\"\"\"\n",
    "    audit = r.get(\"audit\", {})\n",
    "    outcome = r.get(\"outcome\", {})\n",
    "    \n",
    "    # Correctness filter (outcome.status == \"success\")\n",
    "    if REQUIRE_CORRECT and outcome.get(\"status\") != \"success\":\n",
    "        return False\n",
    "    \n",
    "    # Effective difficulty filter (from computation_buckets)\n",
    "    if _effective_difficulty_set is not None:\n",
    "        eff_diff = get_effective_difficulty(r)\n",
    "        if eff_diff is None or eff_diff not in _effective_difficulty_set:\n",
    "            return False\n",
    "    \n",
    "    # Code score filter (audit.code_score)\n",
    "    if _code_scores_set is not None:\n",
    "        code_score = audit.get(\"code_score\")\n",
    "        if code_score is not None and code_score not in _code_scores_set:\n",
    "            return False\n",
    "    \n",
    "    # Token length filter (audit_tokens.completion)\n",
    "    if _token_lengths_set is not None:\n",
    "        tokens = r.get(\"audit_tokens\", {}).get(\"completion\")\n",
    "        if tokens is not None and tokens not in _token_lengths_set:\n",
    "            return False\n",
    "    \n",
    "    # Solution token range filter (problem.original_solution)\n",
    "    if SOLUTION_TOKEN_RANGE is not None:\n",
    "        solution = r.get(\"problem\", {}).get(\"original_solution\") or \"\"\n",
    "        sol_tokens = _estimate_tokens(solution)\n",
    "        lo, hi = SOLUTION_TOKEN_RANGE\n",
    "        if sol_tokens < lo or sol_tokens > hi:\n",
    "            return False\n",
    "    \n",
    "    # ── Math structure filters ─────────────────────────────────────────\n",
    "    ms = r.get(\"math_structure\") or {}\n",
    "    from_text = ms.get(\"from_text\") or {}\n",
    "    from_solution = ms.get(\"from_solution\") or {}\n",
    "    \n",
    "    # Constraint count filter (math_structure.from_text.constraints)\n",
    "    if MAX_CONSTRAINT_COUNT is not None:\n",
    "        constraints = from_text.get(\"constraints\") or []\n",
    "        if len(constraints) > MAX_CONSTRAINT_COUNT:\n",
    "            return False\n",
    "    \n",
    "    # Object count filter (math_structure.from_text.objects)\n",
    "    if MAX_OBJECT_COUNT is not None:\n",
    "        objects = from_text.get(\"objects\") or []\n",
    "        if len(objects) > MAX_OBJECT_COUNT:\n",
    "            return False\n",
    "    \n",
    "    # Reasoning depth filter (math_structure.from_solution.reasoning_depth)\n",
    "    if _reasoning_depth_set is not None:\n",
    "        reasoning_depth = from_solution.get(\"reasoning_depth\")\n",
    "        if reasoning_depth is not None and reasoning_depth not in _reasoning_depth_set:\n",
    "            return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "before = len(records)\n",
    "records = [r for r in records if passes_filters(r)]\n",
    "after = len(records)\n",
    "\n",
    "# Annotate records with effective_difficulty for downstream use\n",
    "for r in records:\n",
    "    r[\"_effective_difficulty\"] = get_effective_difficulty(r)\n",
    "\n",
    "print(f\"After filtering: {before:,} -> {after:,}  (dropped {before - after:,})\")\n",
    "print(\"\\nActive filters:\")\n",
    "print(f\"  - Require correct: {REQUIRE_CORRECT}\")\n",
    "print(f\"  - Effective difficulty: {EFFECTIVE_DIFFICULTY or 'all'}\")\n",
    "print(f\"  - Code scores: {CODE_SCORES or 'all'}\")\n",
    "if TOKEN_LENGTHS:\n",
    "    print(f\"  - Token lengths: [{min(TOKEN_LENGTHS)}, {max(TOKEN_LENGTHS)}] ({len(TOKEN_LENGTHS)} values)\")\n",
    "else:\n",
    "    print(\"  - Token lengths: all\")\n",
    "if SOLUTION_TOKEN_RANGE:\n",
    "    print(f\"  - Solution tokens: [{SOLUTION_TOKEN_RANGE[0]}, {SOLUTION_TOKEN_RANGE[1]}] (estimated, ~4 chars/token)\")\n",
    "else:\n",
    "    print(\"  - Solution tokens: all\")\n",
    "print(f\"  - Max constraint count: {MAX_CONSTRAINT_COUNT or 'all'}\")\n",
    "print(f\"  - Max object count: {MAX_OBJECT_COUNT or 'all'}\")\n",
    "print(f\"  - Reasoning depth: {REASONING_DEPTH or 'all'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available records by domain (after filtering):\n",
      "  algebra: 5,598\n",
      "  combinatorics: 283\n",
      "  geometry: 82\n",
      "  number_theory: 34\n"
     ]
    }
   ],
   "source": [
    "# ── Show available data by domain ──────────────────────────────────────────\n",
    "\n",
    "import random\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "\n",
    "def get_domain(r: dict) -> str:\n",
    "    \"\"\"Null-safe domain extraction from math_structure.from_text.domain.\"\"\"\n",
    "    ms = r.get(\"math_structure\") or {}\n",
    "    ft = ms.get(\"from_text\") or {}\n",
    "    return ft.get(\"domain\", \"unknown\")\n",
    "\n",
    "\n",
    "# Group records by domain\n",
    "by_domain: dict[str, list] = {}\n",
    "for r in records:\n",
    "    domain = get_domain(r)\n",
    "    by_domain.setdefault(domain, []).append(r)\n",
    "\n",
    "print(\"Available records by domain (after filtering):\")\n",
    "for domain, recs in sorted(by_domain.items(), key=lambda x: -len(x[1])):\n",
    "    print(f\"  {domain}: {len(recs):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain distribution analysis:\n",
      "  User target:            20,000\n",
      "  Listed domains total:   5,997\n",
      "  Max proportional target: 3,400 (bottleneck: number_theory with 34 records @ 1%)\n",
      "  Effective target:       3,400\n",
      "\n",
      "  NOTE: Capping at 3,400 to maintain exact distribution proportions.\n",
      "\n",
      "Allocation (target: 3,400):\n",
      "  algebra: 3,060 / 5,598 available [90.0% actual vs 90% desired]\n",
      "  combinatorics: 170 / 283 available [5.0% actual vs 5% desired]\n",
      "  geometry: 68 / 82 available [2.0% actual vs 2% desired]\n",
      "  number_theory: 34 / 34 available (ALL) [1.0% actual vs 1% desired]\n",
      "\n",
      "  Total allocated: 3,400 / 3,400\n",
      "\n",
      "  Unused records: 2,597 from listed domains, 0 from unlisted domains (2,597 total)\n",
      "\n",
      "Final dataset size: 3,400\n"
     ]
    }
   ],
   "source": [
    "# ── Apply domain distribution ──────────────────────────────────────────────\n",
    "\n",
    "selected_records = []\n",
    "\n",
    "if DOMAIN_DISTRIBUTION is not None:\n",
    "    # Validate percentages\n",
    "    total_pct = sum(DOMAIN_DISTRIBUTION.values())\n",
    "    if total_pct > 100:\n",
    "        raise ValueError(f\"Domain percentages sum to {total_pct}%, must be <= 100%\")\n",
    "    \n",
    "    # Determine target size for percentage calculation\n",
    "    target_size = TARGET_TOTAL_RECORDS or len(records)\n",
    "    \n",
    "    # Compute the maximum target that maintains exact distribution proportions.\n",
    "    # The bottleneck domain (fewest records relative to its desired %) caps the total.\n",
    "    max_proportional_targets = []\n",
    "    for domain, pct in DOMAIN_DISTRIBUTION.items():\n",
    "        available = len(by_domain.get(domain, []))\n",
    "        if pct > 0:\n",
    "            max_for_domain = int(available / (pct / 100))\n",
    "            max_proportional_targets.append((domain, max_for_domain, available, pct))\n",
    "    \n",
    "    # Sort to find bottleneck\n",
    "    max_proportional_targets.sort(key=lambda x: x[1])\n",
    "    bottleneck_domain, max_achievable, bn_available, bn_pct = max_proportional_targets[0]\n",
    "    \n",
    "    # Effective target: min of user target, total available (4 listed domains), and max achievable\n",
    "    listed_available = sum(len(by_domain.get(d, [])) for d in DOMAIN_DISTRIBUTION)\n",
    "    effective_target = min(target_size, listed_available, max_achievable)\n",
    "    \n",
    "    print(f\"Domain distribution analysis:\")\n",
    "    print(f\"  User target:            {target_size:,}\")\n",
    "    print(f\"  Listed domains total:   {listed_available:,}\")\n",
    "    print(f\"  Max proportional target: {max_achievable:,} (bottleneck: {bottleneck_domain} \"\n",
    "          f\"with {bn_available:,} records @ {bn_pct}%)\")\n",
    "    print(f\"  Effective target:       {effective_target:,}\\n\")\n",
    "    \n",
    "    if effective_target < target_size:\n",
    "        print(f\"  NOTE: Capping at {effective_target:,} to maintain exact distribution proportions.\\n\")\n",
    "    \n",
    "    # Allocate per domain using effective target\n",
    "    domain_allocated = {}\n",
    "    total_allocated = 0\n",
    "    \n",
    "    print(f\"Allocation (target: {effective_target:,}):\")\n",
    "    for domain, pct in DOMAIN_DISTRIBUTION.items():\n",
    "        available = by_domain.get(domain, [])\n",
    "        desired = int(effective_target * pct / 100)\n",
    "        allocated = min(desired, len(available))\n",
    "        domain_allocated[domain] = allocated\n",
    "        total_allocated += allocated\n",
    "        \n",
    "        actual_pct = (allocated / effective_target * 100) if effective_target > 0 else 0\n",
    "        tag = \" (ALL)\" if allocated == len(available) else \"\"\n",
    "        print(f\"  {domain}: {allocated:,} / {len(available):,} available{tag} \"\n",
    "              f\"[{actual_pct:.1f}% actual vs {pct}% desired]\")\n",
    "    \n",
    "    # Handle rounding remainder: distribute 1 record at a time to domains with capacity\n",
    "    remainder = effective_target - total_allocated\n",
    "    if remainder > 0:\n",
    "        for domain, pct in sorted(DOMAIN_DISTRIBUTION.items(), key=lambda x: -x[1]):\n",
    "            if remainder == 0:\n",
    "                break\n",
    "            available = len(by_domain.get(domain, []))\n",
    "            capacity = available - domain_allocated[domain]\n",
    "            add = min(remainder, capacity)\n",
    "            if add > 0:\n",
    "                domain_allocated[domain] += add\n",
    "                total_allocated += add\n",
    "                remainder -= add\n",
    "    \n",
    "    print(f\"\\n  Total allocated: {total_allocated:,} / {effective_target:,}\")\n",
    "    \n",
    "    # Sample from each domain\n",
    "    for domain, count in domain_allocated.items():\n",
    "        if count > 0:\n",
    "            available = by_domain.get(domain, [])\n",
    "            if count >= len(available):\n",
    "                selected_records.extend(available)\n",
    "            else:\n",
    "                selected_records.extend(random.sample(available, count))\n",
    "    \n",
    "    # Report unused records\n",
    "    unused_listed = listed_available - total_allocated\n",
    "    unused_other = sum(len(v) for d, v in by_domain.items() if d not in DOMAIN_DISTRIBUTION)\n",
    "    if unused_listed > 0 or unused_other > 0:\n",
    "        print(f\"\\n  Unused records: {unused_listed:,} from listed domains, \"\n",
    "              f\"{unused_other:,} from unlisted domains ({unused_listed + unused_other:,} total)\")\n",
    "else:\n",
    "    # No domain distribution specified - take all\n",
    "    selected_records = records\n",
    "    print(\"No domain distribution specified - using all filtered records\")\n",
    "\n",
    "# Shuffle to mix domains\n",
    "random.shuffle(selected_records)\n",
    "records = selected_records\n",
    "\n",
    "print(f\"\\nFinal dataset size: {len(records):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available records by dataset source (after previous filters):\n",
      "  gsm8k: 2,711\n",
      "  numina1.5: 510\n",
      "  mvidia_reasoning_steps: 179\n",
      "\n",
      "Dataset distribution analysis:\n",
      "  Pool size:              3,400\n",
      "  Listed datasets total:  3,400\n",
      "  Max proportional target: 3,388 (bottleneck: gsm8k with 2,711 records @ 80%)\n",
      "  Effective target:       3,388\n",
      "\n",
      "  NOTE: Capping at 3,388 to maintain exact dataset proportions.\n",
      "\n",
      "Allocation (target: 3,388):\n",
      "  gsm8k: 2,710 / 2,711 available [80.0% actual vs 80% desired]\n",
      "  numina1.5: 508 / 510 available [15.0% actual vs 15% desired]\n",
      "  mvidia_reasoning_steps: 169 / 179 available [5.0% actual vs 5% desired]\n",
      "\n",
      "  Total allocated: 3,388 / 3,388\n",
      "\n",
      "  Unused records: 12 from listed datasets, 0 from unlisted datasets (12 total)\n",
      "\n",
      "Dataset size after dataset distribution: 3,388\n"
     ]
    }
   ],
   "source": [
    "# ── Apply dataset source distribution ──────────────────────────────────────\n",
    "\n",
    "# Group current records by dataset source\n",
    "by_dataset: dict[str, list] = {}\n",
    "for r in records:\n",
    "    ds = r.get(\"dataset\", \"unknown\")\n",
    "    by_dataset.setdefault(ds, []).append(r)\n",
    "\n",
    "print(\"Available records by dataset source (after previous filters):\")\n",
    "for ds, recs in sorted(by_dataset.items(), key=lambda x: -len(x[1])):\n",
    "    print(f\"  {ds}: {len(recs):,}\")\n",
    "\n",
    "if DATASET_DISTRIBUTION is not None:\n",
    "    # Validate percentages\n",
    "    total_pct = sum(DATASET_DISTRIBUTION.values())\n",
    "    if total_pct > 100:\n",
    "        raise ValueError(f\"Dataset percentages sum to {total_pct}%, must be <= 100%\")\n",
    "\n",
    "    pool_size = len(records)\n",
    "\n",
    "    # Find the max total that maintains exact proportions (bottleneck analysis)\n",
    "    max_proportional_targets = []\n",
    "    for ds_name, pct in DATASET_DISTRIBUTION.items():\n",
    "        available = len(by_dataset.get(ds_name, []))\n",
    "        if pct > 0:\n",
    "            max_for_ds = int(available / (pct / 100))\n",
    "            max_proportional_targets.append((ds_name, max_for_ds, available, pct))\n",
    "\n",
    "    max_proportional_targets.sort(key=lambda x: x[1])\n",
    "    bn_ds, max_achievable, bn_available, bn_pct = max_proportional_targets[0]\n",
    "\n",
    "    listed_available = sum(len(by_dataset.get(d, [])) for d in DATASET_DISTRIBUTION)\n",
    "    effective_target = min(pool_size, listed_available, max_achievable)\n",
    "\n",
    "    print(f\"\\nDataset distribution analysis:\")\n",
    "    print(f\"  Pool size:              {pool_size:,}\")\n",
    "    print(f\"  Listed datasets total:  {listed_available:,}\")\n",
    "    print(f\"  Max proportional target: {max_achievable:,} (bottleneck: {bn_ds} \"\n",
    "          f\"with {bn_available:,} records @ {bn_pct}%)\")\n",
    "    print(f\"  Effective target:       {effective_target:,}\")\n",
    "\n",
    "    if effective_target < pool_size:\n",
    "        print(f\"\\n  NOTE: Capping at {effective_target:,} to maintain exact dataset proportions.\")\n",
    "\n",
    "    # Allocate per dataset\n",
    "    ds_allocated = {}\n",
    "    total_allocated = 0\n",
    "\n",
    "    print(f\"\\nAllocation (target: {effective_target:,}):\")\n",
    "    for ds_name, pct in DATASET_DISTRIBUTION.items():\n",
    "        available = by_dataset.get(ds_name, [])\n",
    "        desired = int(effective_target * pct / 100)\n",
    "        allocated = min(desired, len(available))\n",
    "        ds_allocated[ds_name] = allocated\n",
    "        total_allocated += allocated\n",
    "\n",
    "        actual_pct = (allocated / effective_target * 100) if effective_target > 0 else 0\n",
    "        tag = \" (ALL)\" if allocated == len(available) else \"\"\n",
    "        print(f\"  {ds_name}: {allocated:,} / {len(available):,} available{tag} \"\n",
    "              f\"[{actual_pct:.1f}% actual vs {pct}% desired]\")\n",
    "\n",
    "    # Distribute rounding remainder\n",
    "    remainder = effective_target - total_allocated\n",
    "    if remainder > 0:\n",
    "        for ds_name, pct in sorted(DATASET_DISTRIBUTION.items(), key=lambda x: -x[1]):\n",
    "            if remainder == 0:\n",
    "                break\n",
    "            available = len(by_dataset.get(ds_name, []))\n",
    "            capacity = available - ds_allocated[ds_name]\n",
    "            add = min(remainder, capacity)\n",
    "            if add > 0:\n",
    "                ds_allocated[ds_name] += add\n",
    "                total_allocated += add\n",
    "                remainder -= add\n",
    "\n",
    "    print(f\"\\n  Total allocated: {total_allocated:,} / {effective_target:,}\")\n",
    "\n",
    "    # Sample from each dataset\n",
    "    ds_selected = []\n",
    "    for ds_name, count in ds_allocated.items():\n",
    "        if count > 0:\n",
    "            available = by_dataset.get(ds_name, [])\n",
    "            if count >= len(available):\n",
    "                ds_selected.extend(available)\n",
    "            else:\n",
    "                ds_selected.extend(random.sample(available, count))\n",
    "\n",
    "    # Report unused\n",
    "    unused_listed = listed_available - total_allocated\n",
    "    unused_other = sum(len(v) for d, v in by_dataset.items() if d not in DATASET_DISTRIBUTION)\n",
    "    if unused_listed > 0 or unused_other > 0:\n",
    "        print(f\"\\n  Unused records: {unused_listed:,} from listed datasets, \"\n",
    "              f\"{unused_other:,} from unlisted datasets ({unused_listed + unused_other:,} total)\")\n",
    "\n",
    "    random.shuffle(ds_selected)\n",
    "    records = ds_selected\n",
    "    print(f\"\\nDataset size after dataset distribution: {len(records):,}\")\n",
    "else:\n",
    "    print(\"\\nNo dataset distribution specified - keeping all records\")\n",
    "    print(f\"Current dataset size: {len(records):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset validation:\n",
      "  Target: 20000\n",
      "  Actual: 3,388\n",
      "  ⚠️  Shortfall: 16,612 records (16.9% of target)\n"
     ]
    }
   ],
   "source": [
    "# ── Validation: Final size check ──────────────────────────────────────────\n",
    "\n",
    "# This cell validates the final dataset size\n",
    "# (The new sampling algorithm in the previous cell should have already handled sizing)\n",
    "\n",
    "print(\"Final dataset validation:\")\n",
    "print(f\"  Target: {TARGET_TOTAL_RECORDS or 'unlimited'}\")\n",
    "print(f\"  Actual: {len(records):,}\")\n",
    "\n",
    "if TARGET_TOTAL_RECORDS is not None:\n",
    "    if len(records) == TARGET_TOTAL_RECORDS:\n",
    "        print(\"  ✓ Target achieved\")\n",
    "    elif len(records) < TARGET_TOTAL_RECORDS:\n",
    "        shortfall = TARGET_TOTAL_RECORDS - len(records)\n",
    "        pct = (len(records) / TARGET_TOTAL_RECORDS) * 100\n",
    "        print(f\"  ⚠️  Shortfall: {shortfall:,} records ({pct:.1f}% of target)\")\n",
    "    else:\n",
    "        # This shouldn't happen with the new algorithm, but safety check\n",
    "        print(f\"  ⚠️  WARNING: Exceeded target by {len(records) - TARGET_TOTAL_RECORDS:,} records\")\n",
    "        print(\"     Downsampling to target size...\")\n",
    "        records = random.sample(records, TARGET_TOTAL_RECORDS)\n",
    "        print(f\"  ✓ Downsampled to {len(records):,} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stratification groups:\n",
      "  algebra_D0: 26\n",
      "  algebra_D1: 447\n",
      "  algebra_D2: 1,742\n",
      "  algebra_D3: 902\n",
      "  combinatorics_D0: 2\n",
      "  combinatorics_D1: 18\n",
      "  combinatorics_D2: 90\n",
      "  combinatorics_D3: 59\n",
      "  geometry_D0: 2\n",
      "  geometry_D1: 6\n",
      "  geometry_D2: 30\n",
      "  geometry_D3: 30\n",
      "  number_theory_D0: 4\n",
      "  number_theory_D1: 6\n",
      "  number_theory_D2: 9\n",
      "  number_theory_D3: 15\n",
      "\n",
      "Train: 3,049\n",
      "Test:  339\n"
     ]
    }
   ],
   "source": [
    "# ── Build stratification key & split ───────────────────────────────────────\n",
    "\n",
    "# Build stratification keys based on domain + effective_difficulty\n",
    "# Uses the null-safe get_domain() helper defined in cell 5\n",
    "strat_keys = []\n",
    "for r in records:\n",
    "    domain = get_domain(r)\n",
    "    eff_diff = r.get(\"_effective_difficulty\", \"NA\")\n",
    "    strat_keys.append(f\"{domain}_D{eff_diff}\")\n",
    "\n",
    "print(\"Stratification groups:\")\n",
    "for k, v in sorted(Counter(strat_keys).items()):\n",
    "    print(f\"  {k}: {v:,}\")\n",
    "\n",
    "# Handle singleton strata (merge into fallback group)\n",
    "strat_counts = Counter(strat_keys)\n",
    "safe_keys = [\n",
    "    k if strat_counts[k] >= 2 else \"_rare_\"\n",
    "    for k in strat_keys\n",
    "]\n",
    "\n",
    "rare_count = sum(1 for k in safe_keys if k == \"_rare_\")\n",
    "if rare_count:\n",
    "    print(f\"\\nMerged {rare_count} record(s) from singleton strata into '_rare_' group\")\n",
    "\n",
    "# Stratified split\n",
    "indices = list(range(len(records)))\n",
    "\n",
    "train_idx, test_idx = train_test_split(\n",
    "    indices,\n",
    "    test_size=TEST_RATIO,\n",
    "    random_state=RANDOM_SEED,\n",
    "    stratify=safe_keys,\n",
    ")\n",
    "\n",
    "train_records = [records[i] for i in train_idx]\n",
    "test_records = [records[i] for i in test_idx]\n",
    "\n",
    "print(f\"\\nTrain: {len(train_records):,}\")\n",
    "print(f\"Test:  {len(test_records):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>total</th>\n",
       "      <th>train</th>\n",
       "      <th>test</th>\n",
       "      <th>train_%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>algebra_D0</td>\n",
       "      <td>26</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>88.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>algebra_D1</td>\n",
       "      <td>447</td>\n",
       "      <td>402</td>\n",
       "      <td>45</td>\n",
       "      <td>89.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>algebra_D2</td>\n",
       "      <td>1742</td>\n",
       "      <td>1568</td>\n",
       "      <td>174</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>algebra_D3</td>\n",
       "      <td>902</td>\n",
       "      <td>812</td>\n",
       "      <td>90</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>combinatorics_D0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>combinatorics_D1</td>\n",
       "      <td>18</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>88.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>combinatorics_D2</td>\n",
       "      <td>90</td>\n",
       "      <td>81</td>\n",
       "      <td>9</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>combinatorics_D3</td>\n",
       "      <td>59</td>\n",
       "      <td>53</td>\n",
       "      <td>6</td>\n",
       "      <td>89.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>geometry_D0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>geometry_D1</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>83.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>geometry_D2</td>\n",
       "      <td>30</td>\n",
       "      <td>27</td>\n",
       "      <td>3</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>geometry_D3</td>\n",
       "      <td>30</td>\n",
       "      <td>27</td>\n",
       "      <td>3</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>number_theory_D0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>number_theory_D1</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>83.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>number_theory_D2</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>88.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>number_theory_D3</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>93.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               group  total  train  test  train_%\n",
       "0         algebra_D0     26     23     3     88.5\n",
       "1         algebra_D1    447    402    45     89.9\n",
       "2         algebra_D2   1742   1568   174     90.0\n",
       "3         algebra_D3    902    812    90     90.0\n",
       "4   combinatorics_D0      2      2     0    100.0\n",
       "5   combinatorics_D1     18     16     2     88.9\n",
       "6   combinatorics_D2     90     81     9     90.0\n",
       "7   combinatorics_D3     59     53     6     89.8\n",
       "8        geometry_D0      2      2     0    100.0\n",
       "9        geometry_D1      6      5     1     83.3\n",
       "10       geometry_D2     30     27     3     90.0\n",
       "11       geometry_D3     30     27     3     90.0\n",
       "12  number_theory_D0      4      4     0    100.0\n",
       "13  number_theory_D1      6      5     1     83.3\n",
       "14  number_theory_D2      9      8     1     88.9\n",
       "15  number_theory_D3     15     14     1     93.3"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ── Verify stratification proportions ──────────────────────────────────────\n",
    "\n",
    "train_strat = Counter(strat_keys[i] for i in train_idx)\n",
    "test_strat = Counter(strat_keys[i] for i in test_idx)\n",
    "all_groups = sorted(set(strat_keys))\n",
    "\n",
    "rows = []\n",
    "for g in all_groups:\n",
    "    total = train_strat.get(g, 0) + test_strat.get(g, 0)\n",
    "    rows.append({\n",
    "        \"group\": g,\n",
    "        \"total\": total,\n",
    "        \"train\": train_strat.get(g, 0),\n",
    "        \"test\": test_strat.get(g, 0),\n",
    "        \"train_%\": round(train_strat.get(g, 0) / total * 100, 1) if total else 0,\n",
    "    })\n",
    "\n",
    "pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples per dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>total</th>\n",
       "      <th>train</th>\n",
       "      <th>test</th>\n",
       "      <th>pct_of_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gsm8k</td>\n",
       "      <td>2711</td>\n",
       "      <td>2437</td>\n",
       "      <td>274</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>numina1.5</td>\n",
       "      <td>508</td>\n",
       "      <td>459</td>\n",
       "      <td>49</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mvidia_reasoning_steps</td>\n",
       "      <td>169</td>\n",
       "      <td>153</td>\n",
       "      <td>16</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  dataset  total  train  test  pct_of_total\n",
       "0                   gsm8k   2711   2437   274          80.0\n",
       "1               numina1.5    508    459    49          15.0\n",
       "2  mvidia_reasoning_steps    169    153    16           5.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ── Samples per dataset (train / test) ─────────────────────────────────────\n",
    "\n",
    "train_ds = Counter(r.get(\"dataset\", \"unknown\") for r in train_records)\n",
    "test_ds = Counter(r.get(\"dataset\", \"unknown\") for r in test_records)\n",
    "all_datasets = sorted(set(train_ds) | set(test_ds))\n",
    "\n",
    "ds_rows = []\n",
    "for ds in all_datasets:\n",
    "    total = train_ds.get(ds, 0) + test_ds.get(ds, 0)\n",
    "    ds_rows.append({\n",
    "        \"dataset\": ds,\n",
    "        \"total\": total,\n",
    "        \"train\": train_ds.get(ds, 0),\n",
    "        \"test\": test_ds.get(ds, 0),\n",
    "        \"pct_of_total\": round(total / len(records) * 100, 1),\n",
    "    })\n",
    "\n",
    "# Sort by total descending\n",
    "ds_rows.sort(key=lambda x: -x[\"total\"])\n",
    "\n",
    "print(\"Samples per dataset:\")\n",
    "pd.DataFrame(ds_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 3,049 train records -> /home/larcanio/AIMO3_v2/data/datasets/splits/algebra_specialist/train.jsonl\n",
      "Saved 339 test records  -> /home/larcanio/AIMO3_v2/data/datasets/splits/algebra_specialist/test.jsonl\n",
      "\n",
      "============================================================\n",
      "DATASET SUMMARY\n",
      "============================================================\n",
      "Sources: 1 datasets\n",
      "Filters: correct=True\n",
      "         effective_difficulty=[0, 1, 2, 3]\n",
      "         code_scores=all\n",
      "         tokens=[0-511]\n",
      "         solution_tokens=all\n",
      "         max_constraint_count=3\n",
      "         max_object_count=3\n",
      "         reasoning_depth=['shallow', 'medium']\n",
      "Domains: algebra:90%, combinatorics:5%, geometry:2%, number_theory:1%\n",
      "Datasets: gsm8k:80%, numina1.5:15%, mvidia_reasoning_steps:5%\n",
      "Target:  20000\n",
      "Split:   90% train / 10% test\n",
      "Output:  /home/larcanio/AIMO3_v2/data/datasets/splits/algebra_specialist\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ── Save to disk ───────────────────────────────────────────────────────────\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "train_path = os.path.join(OUTPUT_DIR, \"train.jsonl\")\n",
    "test_path = os.path.join(OUTPUT_DIR, \"test.jsonl\")\n",
    "\n",
    "with open(train_path, \"w\") as f:\n",
    "    for r in train_records:\n",
    "        f.write(json.dumps(r) + \"\\n\")\n",
    "\n",
    "with open(test_path, \"w\") as f:\n",
    "    for r in test_records:\n",
    "        f.write(json.dumps(r) + \"\\n\")\n",
    "\n",
    "print(f\"Saved {len(train_records):,} train records -> {train_path}\")\n",
    "print(f\"Saved {len(test_records):,} test records  -> {test_path}\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"DATASET SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Sources: {len(DATASET_PATHS)} datasets\")\n",
    "print(f\"Filters: correct={REQUIRE_CORRECT}\")\n",
    "print(f\"         effective_difficulty={EFFECTIVE_DIFFICULTY or 'all'}\")\n",
    "print(f\"         code_scores={CODE_SCORES or 'all'}\")\n",
    "if TOKEN_LENGTHS:\n",
    "    print(f\"         tokens=[{min(TOKEN_LENGTHS)}-{max(TOKEN_LENGTHS)}]\")\n",
    "else:\n",
    "    print(\"         tokens=all\")\n",
    "if SOLUTION_TOKEN_RANGE:\n",
    "    print(f\"         solution_tokens=[{SOLUTION_TOKEN_RANGE[0]}-{SOLUTION_TOKEN_RANGE[1]}]\")\n",
    "else:\n",
    "    print(\"         solution_tokens=all\")\n",
    "print(f\"         max_constraint_count={MAX_CONSTRAINT_COUNT or 'all'}\")\n",
    "print(f\"         max_object_count={MAX_OBJECT_COUNT or 'all'}\")\n",
    "print(f\"         reasoning_depth={REASONING_DEPTH or 'all'}\")\n",
    "if DOMAIN_DISTRIBUTION:\n",
    "    print(f\"Domains: {', '.join(f'{d}:{p}%' for d, p in DOMAIN_DISTRIBUTION.items())}\")\n",
    "else:\n",
    "    print(\"Domains: all (no distribution)\")\n",
    "if DATASET_DISTRIBUTION:\n",
    "    print(f\"Datasets: {', '.join(f'{d}:{p}%' for d, p in DATASET_DISTRIBUTION.items())}\")\n",
    "else:\n",
    "    print(\"Datasets: all (no distribution)\")\n",
    "print(f\"Target:  {TARGET_TOTAL_RECORDS or 'unlimited'}\")\n",
    "print(f\"Split:   {TRAIN_RATIO*100:.0f}% train / {TEST_RATIO*100:.0f}% test\")\n",
    "print(f\"Output:  {OUTPUT_DIR}\")\n",
    "print(f\"{'='*60}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
