{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dataset Split: Train / Test (Normalized Format)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "from typing import Optional\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "\n",
        "# Source JSONL files to load and concatenate\n",
        "DATASET_PATH = \"/home/larcanio/AIMO3_v2/data/datasets/Dataset_Full/bucketed/dataset_full_metadata.jsonl\"\n",
        "\n",
        "# Output directory\n",
        "OUTPUT_DIR = \"/home/larcanio/AIMO3_v2/data/datasets/splits/gemma_balanced\"\n",
        "\n",
        "# Filtering Options\n",
        "\n",
        "# Only include records where outcome.status == \"success\"\n",
        "REQUIRE_CORRECT = True\n",
        "\n",
        "# Effective difficulty distribution\n",
        "# Key = bucket level (or None for unsolved), value = % of final dataset.\n",
        "# Percentages must sum to 100 to preserve proportions.\n",
        "# Levels not listed will be excluded.\n",
        "# Derived from computation_buckets: min level where passes >= 1\n",
        "# Available levels: 0, 1, 2, 3, 4, 5, 6, 7, None (unsolved)\n",
        "EFFECTIVE_DIFFICULTY: Optional[dict[int | None, int]] = {\n",
        "    2: 34,\n",
        "    3: 33,\n",
        "    4: 33,\n",
        "}\n",
        "\n",
        "# Code scores to include (None = all scores)\n",
        "CODE_SCORES: Optional[list[int]] = None\n",
        "\n",
        "# Solution token range: (min, max) estimated tokens for problem.original_solution\n",
        "# Estimated as len(text) // 4. Set to None to disable.\n",
        "SOLUTION_TOKEN_RANGE: Optional[tuple[int, int]] = None  # e.g. (0, 500)\n",
        "\n",
        "# Math Structure Filters\n",
        "# Filters based on math_structure.from_text and math_structure.from_solution\n",
        "# Set to None to disable a filter.\n",
        "\n",
        "# Max number of constraints in math_structure.from_text.constraints (None = no limit)\n",
        "MAX_CONSTRAINT_COUNT: Optional[int] = 1\n",
        "\n",
        "# Max number of objects in math_structure.from_text.objects (None = no limit)\n",
        "MAX_OBJECT_COUNT: Optional[int] = 1\n",
        "\n",
        "# Tag/value filters (empty list = no filtering)\n",
        "#\n",
        "# NOTE: these filter at the record level (\"must match\") and are applied BEFORE\n",
        "# the % distributions / downsampling.\n",
        "\n",
        "# Objects tags from math_structure.from_text.objects (list[str])\n",
        "# EDA note: rich vocab (~320 types, ~99% populated). Example tag seen in sample: \"positive_integer\".\n",
        "OBJECTS_FILTER: list[str] = []  # e.g. [\"positive_integer\"]\n",
        "\n",
        "# Constraints tags from math_structure.from_text.constraints (list[str])\n",
        "# Common values (from EDA, N >= 500):\n",
        "#   equality, exists, inequality, distinct, forall, bounded, divisibility, parity,\n",
        "#   positive_integer, integral\n",
        "CONSTRAINTS_FILTER: list[str] = []  # e.g. [\"equality\", \"divisibility\"]\n",
        "\n",
        "# Technique transitions from math_structure.from_solution.technique_transitions (scalar)\n",
        "# Values (from EDA): 0, 1, 2, 3  (rare literal \"null\" also appears)\n",
        "TECHNIQUE_TRANSITIONS_FILTER: list[int] = []  # e.g. [0, 1]\n",
        "\n",
        "# Reasoning scope from math_structure.from_solution.reasoning_scope (scalar)\n",
        "# Values (from EDA): \"local\", \"global\", \"mixed\", \"none\"  (rare literal \"null\" also appears)\n",
        "REASONING_SCOPE_FILTER: list[str] = []  # e.g. [\"local\", \"global\"]\n",
        "\n",
        "# Intermediate reuse from math_structure.from_solution.intermediate_reuse (scalar)\n",
        "# Values (from EDA): \"none\", \"single\", \"multiple\"  (rare literal \"null\" also appears)\n",
        "INTERMEDIATE_REUSE_FILTER: list[str] = []  # e.g. [\"none\", \"single\"]\n",
        "\n",
        "# Reasoning depth distribution\n",
        "# Key = depth string, value = % of final dataset.\n",
        "# Percentages must sum to 100 to preserve proportions.\n",
        "# Depths not listed will be excluded.\n",
        "# Extracted from math_structure.from_solution.reasoning_depth (missing -> \"unknown\")\n",
        "REASONING_DEPTH: Optional[dict[str, int]] = {\n",
        "    \"shallow\": 50,\n",
        "    \"medium\": 50,\n",
        "}\n",
        "\n",
        "# Domain Distribution\n",
        "# Specify percentage (1-100) of final dataset per domain.\n",
        "# Percentages should sum to 100. Set to None to use all domains equally.\n",
        "# Domains not listed will be excluded.\n",
        "\n",
        "DOMAIN_DISTRIBUTION: Optional[dict[str, int]] = {\n",
        "    \"algebra\": 65,\n",
        "    \"combinatorics\": 20,\n",
        "    \"geometry\": 5,\n",
        "    \"number_theory\": 10,\n",
        "    \n",
        "}\n",
        "\n",
        "# Dataset Source Distribution\n",
        "# Specify percentage (1-100) of final dataset per source dataset (record[\"dataset\"]).\n",
        "# Percentages should sum to <= 100. Set to None to keep all records (no resampling).\n",
        "# Applied after domain distribution.\n",
        "# Available datasets: gsm8k, numina1.5, mvidia_reasoning_steps,\n",
        "#   numina1.5:aops, numina1.5:cn_contest, numina1.5:metamath,\n",
        "#   numina1.5:inequalities, numina1.5:number_theory\n",
        "\n",
        "# Example:\n",
        "# DATASET_DISTRIBUTION: Optional[dict[str, int]] = {\n",
        "#     \"gsm8k\": 55,\n",
        "#     \"numina1.5\": 15,\n",
        "#     \"mvidia_reasoning_steps\": 20,\n",
        "# }\n",
        "DATASET_DISTRIBUTION = None\n",
        "\n",
        "# Target Dataset Size\n",
        "TARGET_TOTAL_RECORDS: Optional[int] = 15000\n",
        "\n",
        "# Train/Test Split\n",
        "TRAIN_RATIO = 0.9\n",
        "TEST_RATIO = 0.1\n",
        "RANDOM_SEED = 42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded records from bucketed/dataset_full_metadata.jsonl\n",
            "\n",
            "Total records loaded: 71,832\n"
          ]
        }
      ],
      "source": [
        "# ── Load  ─────────────────────────────────────────────────────\n",
        "\n",
        "records = []\n",
        "\n",
        "with open(DATASET_PATH) as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            records.append(json.loads(line))\n",
        "\n",
        "\n",
        "print(f\"Loaded records from {Path(DATASET_PATH).parent.name}/{Path(DATASET_PATH).name}\")\n",
        "\n",
        "print(f\"\\nTotal records loaded: {len(records):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After filtering: 71,832 -> 7,123  (dropped 64,709)\n",
            "\n",
            "Active filters:\n",
            "  - Require correct: True\n",
            "  - Effective difficulty: [2, 3, 4]\n",
            "  - Code scores: all\n",
            "  - Solution tokens: all\n",
            "  - Max constraint count: 1\n",
            "  - Max object count: 1\n",
            "  - Reasoning depth: ['shallow', 'medium']\n"
          ]
        }
      ],
      "source": [
        "# ── Apply filters ──────────────────────────────────────────────────────────\n",
        "\n",
        "# Convert lists to sets for O(1) lookup\n",
        "_code_scores_set = set(CODE_SCORES) if CODE_SCORES else None\n",
        "\n",
        "\n",
        "def get_effective_difficulty(r: dict) -> int | None:\n",
        "    \"\"\"Compute effective difficulty: minimum bucket level where passes >= 1.\"\"\"\n",
        "    buckets = r.get(\"computation_buckets\", [])\n",
        "    passing = [b[\"level\"] for b in buckets if b.get(\"passes\", 0) >= 1]\n",
        "    return min(passing) if passing else None\n",
        "\n",
        "\n",
        "def get_reasoning_depth(r: dict) -> str:\n",
        "    \"\"\"Null-safe reasoning depth from math_structure.from_solution.reasoning_depth.\"\"\"\n",
        "    ms = r.get(\"math_structure\") or {}\n",
        "    fs = ms.get(\"from_solution\") or {}\n",
        "    return fs.get(\"reasoning_depth\") or \"unknown\"\n",
        "\n",
        "\n",
        "def _estimate_tokens(text: str) -> int:\n",
        "    \"\"\"Estimate token count from text (avg ~4 chars per token).\"\"\"\n",
        "    return len(text) // 4\n",
        "\n",
        "\n",
        "def _fmt_pct_distribution(d: Optional[dict]) -> str:\n",
        "    if not d:\n",
        "        return \"all\"\n",
        "    return \", \".join(f\"{k}:{v}%\" for k, v in d.items())\n",
        "\n",
        "\n",
        "def _fmt_list_filter(xs) -> str:\n",
        "    if not xs:\n",
        "        return \"all\"\n",
        "    return \", \".join(map(str, xs))\n",
        "\n",
        "\n",
        "def passes_filters(r: dict) -> bool:\n",
        "    \"\"\"Check if a record passes all configured filters.\"\"\"\n",
        "    audit = r.get(\"audit\", {})\n",
        "    outcome = r.get(\"outcome\", {})\n",
        "\n",
        "    # Correctness filter (outcome.status == \"success\")\n",
        "    if REQUIRE_CORRECT and outcome.get(\"status\") != \"success\":\n",
        "        return False\n",
        "\n",
        "    # Code score filter (audit.code_score)\n",
        "    if _code_scores_set is not None:\n",
        "        code_score = audit.get(\"code_score\")\n",
        "        if code_score is not None and code_score not in _code_scores_set:\n",
        "            return False\n",
        "\n",
        "    # Solution token range filter (problem.original_solution)\n",
        "    if SOLUTION_TOKEN_RANGE is not None:\n",
        "        solution = r.get(\"problem\", {}).get(\"original_solution\") or \"\"\n",
        "        sol_tokens = _estimate_tokens(solution)\n",
        "        lo, hi = SOLUTION_TOKEN_RANGE\n",
        "        if sol_tokens < lo or sol_tokens > hi:\n",
        "            return False\n",
        "\n",
        "    # ── Math structure filters ─────────────────────────────────────────\n",
        "    ms = r.get(\"math_structure\") or {}\n",
        "    from_text = ms.get(\"from_text\") or {}\n",
        "    from_solution = (ms.get(\"from_solution\") or {})\n",
        "\n",
        "    # Constraint count filter (math_structure.from_text.constraints)\n",
        "    if MAX_CONSTRAINT_COUNT is not None:\n",
        "        constraints = from_text.get(\"constraints\") or []\n",
        "        if len(constraints) > MAX_CONSTRAINT_COUNT:\n",
        "            return False\n",
        "\n",
        "    # Object count filter (math_structure.from_text.objects)\n",
        "    if MAX_OBJECT_COUNT is not None:\n",
        "        objects = from_text.get(\"objects\") or []\n",
        "        if len(objects) > MAX_OBJECT_COUNT:\n",
        "            return False\n",
        "\n",
        "    # Tag/value filters (empty list = no filtering)\n",
        "\n",
        "    if OBJECTS_FILTER:\n",
        "        objects = from_text.get(\"objects\") or []\n",
        "        if not any(o in OBJECTS_FILTER for o in objects):\n",
        "            return False\n",
        "\n",
        "    if CONSTRAINTS_FILTER:\n",
        "        constraints = from_text.get(\"constraints\") or []\n",
        "        if not any(c in CONSTRAINTS_FILTER for c in constraints):\n",
        "            return False\n",
        "\n",
        "    if TECHNIQUE_TRANSITIONS_FILTER:\n",
        "        tt = from_solution.get(\"technique_transitions\")\n",
        "        if tt not in TECHNIQUE_TRANSITIONS_FILTER:\n",
        "            return False\n",
        "\n",
        "    if REASONING_SCOPE_FILTER:\n",
        "        scope = from_solution.get(\"reasoning_scope\") or \"unknown\"\n",
        "        if scope not in REASONING_SCOPE_FILTER:\n",
        "            return False\n",
        "\n",
        "    if INTERMEDIATE_REUSE_FILTER:\n",
        "        reuse = from_solution.get(\"intermediate_reuse\") or \"unknown\"\n",
        "        if reuse not in INTERMEDIATE_REUSE_FILTER:\n",
        "            return False\n",
        "\n",
        "    return True\n",
        "\n",
        "before = len(records)\n",
        "records = [r for r in records if passes_filters(r)]\n",
        "after = len(records)\n",
        "\n",
        "# Annotate records with effective_difficulty + reasoning_depth for downstream use\n",
        "for r in records:\n",
        "    r[\"_effective_difficulty\"] = get_effective_difficulty(r)\n",
        "    r[\"_reasoning_depth\"] = get_reasoning_depth(r)\n",
        "\n",
        "print(f\"After filtering: {before:,} -> {after:,}  (dropped {before - after:,})\")\n",
        "print(\"\\nActive filters:\")\n",
        "print(f\"  - Require correct: {REQUIRE_CORRECT}\")\n",
        "print(f\"  - Effective difficulty target: {_fmt_pct_distribution(EFFECTIVE_DIFFICULTY)}\")\n",
        "print(f\"  - Code scores: {CODE_SCORES or 'all'}\")\n",
        "if SOLUTION_TOKEN_RANGE:\n",
        "    print(f\"  - Solution tokens: [{SOLUTION_TOKEN_RANGE[0]}, {SOLUTION_TOKEN_RANGE[1]}] (estimated, ~4 chars/token)\")\n",
        "else:\n",
        "    print(\"  - Solution tokens: all\")\n",
        "print(f\"  - Max constraint count: {MAX_CONSTRAINT_COUNT or 'all'}\")\n",
        "print(f\"  - Max object count: {MAX_OBJECT_COUNT or 'all'}\")\n",
        "print(f\"  - Objects filter: {_fmt_list_filter(OBJECTS_FILTER)}\")\n",
        "print(f\"  - Constraints filter: {_fmt_list_filter(CONSTRAINTS_FILTER)}\")\n",
        "print(f\"  - Technique transitions filter: {_fmt_list_filter(TECHNIQUE_TRANSITIONS_FILTER)}\")\n",
        "print(f\"  - Reasoning scope filter: {_fmt_list_filter(REASONING_SCOPE_FILTER)}\")\n",
        "print(f\"  - Intermediate reuse filter: {_fmt_list_filter(INTERMEDIATE_REUSE_FILTER)}\")\n",
        "print(f\"  - Reasoning depth target: {_fmt_pct_distribution(REASONING_DEPTH)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available records by domain (after filtering):\n",
            "  algebra: 3,581\n",
            "  number_theory: 1,817\n",
            "  combinatorics: 1,211\n",
            "  geometry: 423\n",
            "  mixed: 62\n",
            "  unknown: 23\n",
            "  None: 6\n"
          ]
        }
      ],
      "source": [
        "# ── Apply distributions + show availability ───────────────────────────────\n",
        "\n",
        "import random\n",
        "random.seed(RANDOM_SEED)\n",
        "\n",
        "\n",
        "def get_domain(r: dict) -> str:\n",
        "    \"\"\"Null-safe domain extraction from math_structure.from_text.domain.\"\"\"\n",
        "    ms = r.get(\"math_structure\") or {}\n",
        "    ft = ms.get(\"from_text\") or {}\n",
        "    return ft.get(\"domain\", \"unknown\")\n",
        "\n",
        "\n",
        "def apply_target_distribution(\n",
        "    records: list[dict],\n",
        "    *,\n",
        "    key_fn,\n",
        "    distribution: Optional[dict],\n",
        "    label: str,\n",
        "    target_size: Optional[int],\n",
        ") -> list[dict]:\n",
        "    \"\"\"Downsample records to match distribution exactly (capped at bottleneck).\"\"\"\n",
        "    if distribution is None:\n",
        "        return records\n",
        "\n",
        "    total_pct = sum(distribution.values())\n",
        "    if total_pct != 100:\n",
        "        raise ValueError(f\"{label} percentages sum to {total_pct}%, must be 100%\")\n",
        "\n",
        "    # Partition records into requested keys vs unlisted keys\n",
        "    by_key: dict = {k: [] for k in distribution}\n",
        "    other = []\n",
        "    for r in records:\n",
        "        k = key_fn(r)\n",
        "        if k in by_key:\n",
        "            by_key[k].append(r)\n",
        "        else:\n",
        "            other.append(r)\n",
        "\n",
        "    missing = [k for k, pct in distribution.items() if pct > 0 and len(by_key.get(k, [])) == 0]\n",
        "    if missing:\n",
        "        raise ValueError(f\"{label}: no available records for key(s): {missing}\")\n",
        "\n",
        "    pool_size = len(records)\n",
        "    listed_available = sum(len(v) for v in by_key.values())\n",
        "    \n",
        "    # Domain distribution logic: cap at bottleneck to preserve exact proportions\n",
        "    max_proportional_targets = []\n",
        "    for k, pct in distribution.items():\n",
        "        available = len(by_key.get(k, []))\n",
        "        if pct > 0:\n",
        "            max_for_k = int(available / (pct / 100))\n",
        "            max_proportional_targets.append((k, max_for_k, available, pct))\n",
        "\n",
        "    max_proportional_targets.sort(key=lambda x: x[1])\n",
        "    bn_key, max_achievable, bn_available, bn_pct = max_proportional_targets[0]\n",
        "\n",
        "    requested_target = target_size or pool_size\n",
        "    effective_target = min(requested_target, listed_available, max_achievable)\n",
        "\n",
        "    print(f\"\\n{label} distribution analysis:\")\n",
        "    print(f\"  Pool size:              {pool_size:,}\")\n",
        "    print(f\"  Listed keys total:      {listed_available:,}\")\n",
        "    print(f\"  Unlisted keys excluded: {len(other):,}\")\n",
        "    print(\n",
        "        f\"  Max proportional target: {max_achievable:,} (bottleneck: {bn_key} \"\n",
        "        f\"with {bn_available:,} records @ {bn_pct}%)\"\n",
        "    )\n",
        "    print(f\"  Effective target:       {effective_target:,}\\n\")\n",
        "\n",
        "    allocated: dict = {}\n",
        "    total_allocated = 0\n",
        "\n",
        "    print(f\"Allocation (target: {effective_target:,}):\")\n",
        "    for k, pct in distribution.items():\n",
        "        available = len(by_key.get(k, []))\n",
        "        desired = int(effective_target * pct / 100)\n",
        "        count = min(desired, available)\n",
        "        allocated[k] = count\n",
        "        total_allocated += count\n",
        "\n",
        "        actual_pct = (count / effective_target * 100) if effective_target > 0 else 0\n",
        "        tag = \" (ALL)\" if count == available else \"\"\n",
        "        print(f\"  {k}: {count:,} / {available:,} available{tag} [{actual_pct:.1f}% actual vs {pct}% desired]\")\n",
        "\n",
        "    remainder = effective_target - total_allocated\n",
        "    if remainder > 0:\n",
        "        for k, pct in sorted(distribution.items(), key=lambda x: -x[1]):\n",
        "            if remainder == 0:\n",
        "                break\n",
        "            available = len(by_key.get(k, []))\n",
        "            capacity = available - allocated[k]\n",
        "            add = min(remainder, capacity)\n",
        "            if add > 0:\n",
        "                allocated[k] += add\n",
        "                total_allocated += add\n",
        "                remainder -= add\n",
        "\n",
        "    print(f\"\\n  Total allocated: {total_allocated:,} / {effective_target:,}\")\n",
        "\n",
        "    selected = []\n",
        "    for k, count in allocated.items():\n",
        "        if count <= 0:\n",
        "            continue\n",
        "        available = by_key.get(k, [])\n",
        "        if count >= len(available):\n",
        "            selected.extend(available)\n",
        "        else:\n",
        "            selected.extend(random.sample(available, count))\n",
        "\n",
        "    random.shuffle(selected)\n",
        "    return selected\n",
        "\n",
        "\n",
        "# ── Apply effective difficulty distribution ────────────────────────────────\n",
        "\n",
        "if EFFECTIVE_DIFFICULTY is not None:\n",
        "    print(\"Available records by effective difficulty (after filtering):\")\n",
        "    eff_counts = Counter(r.get(\"_effective_difficulty\") for r in records)\n",
        "    for k, v in sorted(eff_counts.items(), key=lambda x: (x[0] is None, x[0])):\n",
        "        print(f\"  {k}: {v:,}\")\n",
        "\n",
        "    records = apply_target_distribution(\n",
        "        records,\n",
        "        key_fn=lambda r: r.get(\"_effective_difficulty\"),\n",
        "        distribution=EFFECTIVE_DIFFICULTY,\n",
        "        label=\"Effective difficulty\",\n",
        "        target_size=TARGET_TOTAL_RECORDS,\n",
        "    )\n",
        "    print(f\"\\nDataset size after effective difficulty distribution: {len(records):,}\")\n",
        "\n",
        "\n",
        "# ── Apply reasoning depth distribution ─────────────────────────────────────\n",
        "\n",
        "if REASONING_DEPTH is not None:\n",
        "    print(\"\\nAvailable records by reasoning depth (after previous steps):\")\n",
        "    depth_counts = Counter(r.get(\"_reasoning_depth\", \"unknown\") for r in records)\n",
        "    for k, v in sorted(depth_counts.items(), key=lambda x: (-x[1], str(x[0]))):\n",
        "        print(f\"  {k}: {v:,}\")\n",
        "\n",
        "    records = apply_target_distribution(\n",
        "        records,\n",
        "        key_fn=lambda r: r.get(\"_reasoning_depth\", \"unknown\"),\n",
        "        distribution=REASONING_DEPTH,\n",
        "        label=\"Reasoning depth\",\n",
        "        target_size=TARGET_TOTAL_RECORDS,\n",
        "    )\n",
        "    print(f\"\\nDataset size after reasoning depth distribution: {len(records):,}\")\n",
        "\n",
        "\n",
        "# ── Show available data by domain ─────────────────────────────────────────-\n",
        "\n",
        "by_domain: dict[str, list] = {}\n",
        "for r in records:\n",
        "    domain = get_domain(r)\n",
        "    by_domain.setdefault(domain, []).append(r)\n",
        "\n",
        "print(\"\\nAvailable records by domain (after filtering + distributions):\")\n",
        "for domain, recs in sorted(by_domain.items(), key=lambda x: -len(x[1])):\n",
        "    print(f\"  {domain}: {len(recs):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Domain distribution analysis:\n",
            "  User target:            15,000\n",
            "  Listed domains total:   7,032\n",
            "  Max proportional target: 5,509 (bottleneck: algebra with 3,581 records @ 65%)\n",
            "  Effective target:       5,509\n",
            "\n",
            "  NOTE: Capping at 5,509 to maintain exact distribution proportions.\n",
            "\n",
            "Allocation (target: 5,509):\n",
            "  algebra: 3,580 / 3,581 available [65.0% actual vs 65% desired]\n",
            "  combinatorics: 1,101 / 1,211 available [20.0% actual vs 20% desired]\n",
            "  geometry: 275 / 423 available [5.0% actual vs 5% desired]\n",
            "  number_theory: 550 / 1,817 available [10.0% actual vs 10% desired]\n",
            "\n",
            "  Total allocated: 5,509 / 5,509\n",
            "\n",
            "  Unused records: 1,523 from listed domains, 91 from unlisted domains (1,614 total)\n",
            "\n",
            "Final dataset size: 5,509\n"
          ]
        }
      ],
      "source": [
        "# ── Apply domain distribution ──────────────────────────────────────────────\n",
        "\n",
        "selected_records = []\n",
        "\n",
        "if DOMAIN_DISTRIBUTION is not None:\n",
        "    # Validate percentages\n",
        "    total_pct = sum(DOMAIN_DISTRIBUTION.values())\n",
        "    if total_pct > 100:\n",
        "        raise ValueError(f\"Domain percentages sum to {total_pct}%, must be <= 100%\")\n",
        "    \n",
        "    target_size = TARGET_TOTAL_RECORDS or len(records)\n",
        "    \n",
        "    max_proportional_targets = []\n",
        "    for domain, pct in DOMAIN_DISTRIBUTION.items():\n",
        "        available = len(by_domain.get(domain, []))\n",
        "        if pct > 0:\n",
        "            max_for_domain = int(available / (pct / 100))\n",
        "            max_proportional_targets.append((domain, max_for_domain, available, pct))\n",
        "    \n",
        "    max_proportional_targets.sort(key=lambda x: x[1])\n",
        "    bottleneck_domain, max_achievable, bn_available, bn_pct = max_proportional_targets[0]\n",
        "\n",
        "    listed_available = sum(len(by_domain.get(d, [])) for d in DOMAIN_DISTRIBUTION)\n",
        "    effective_target = min(target_size, listed_available, max_achievable)\n",
        "    \n",
        "    print(\"Domain distribution analysis:\")\n",
        "    print(f\"  User target:            {target_size:,}\")\n",
        "    print(f\"  Listed domains total:   {listed_available:,}\")\n",
        "    print(f\"  Max proportional target: {max_achievable:,} (bottleneck: {bottleneck_domain} \"\n",
        "          f\"with {bn_available:,} records @ {bn_pct}%)\")\n",
        "    print(f\"  Effective target:       {effective_target:,}\\n\")\n",
        "    \n",
        "    if effective_target < target_size:\n",
        "        print(f\"  NOTE: Capping at {effective_target:,} to maintain exact distribution proportions.\\n\")\n",
        "    \n",
        "    domain_allocated = {}\n",
        "    total_allocated = 0\n",
        "    \n",
        "    print(f\"Allocation (target: {effective_target:,}):\")\n",
        "    for domain, pct in DOMAIN_DISTRIBUTION.items():\n",
        "        available = by_domain.get(domain, [])\n",
        "        desired = int(effective_target * pct / 100)\n",
        "        allocated = min(desired, len(available))\n",
        "        domain_allocated[domain] = allocated\n",
        "        total_allocated += allocated\n",
        "        \n",
        "        actual_pct = (allocated / effective_target * 100) if effective_target > 0 else 0\n",
        "        tag = \" (ALL)\" if allocated == len(available) else \"\"\n",
        "        print(f\"  {domain}: {allocated:,} / {len(available):,} available{tag} \"\n",
        "              f\"[{actual_pct:.1f}% actual vs {pct}% desired]\")\n",
        "    \n",
        "    remainder = effective_target - total_allocated\n",
        "    if remainder > 0:\n",
        "        for domain, pct in sorted(DOMAIN_DISTRIBUTION.items(), key=lambda x: -x[1]):\n",
        "            if remainder == 0:\n",
        "                break\n",
        "            available = len(by_domain.get(domain, []))\n",
        "            capacity = available - domain_allocated[domain]\n",
        "            add = min(remainder, capacity)\n",
        "            if add > 0:\n",
        "                domain_allocated[domain] += add\n",
        "                total_allocated += add\n",
        "                remainder -= add\n",
        "    \n",
        "    print(f\"\\n  Total allocated: {total_allocated:,} / {effective_target:,}\")\n",
        "    \n",
        "    for domain, count in domain_allocated.items():\n",
        "        if count > 0:\n",
        "            available = by_domain.get(domain, [])\n",
        "            if count >= len(available):\n",
        "                selected_records.extend(available)\n",
        "            else:\n",
        "                selected_records.extend(random.sample(available, count))\n",
        "    \n",
        "    unused_listed = listed_available - total_allocated\n",
        "    unused_other = sum(len(v) for d, v in by_domain.items() if d not in DOMAIN_DISTRIBUTION)\n",
        "    if unused_listed > 0 or unused_other > 0:\n",
        "        print(f\"\\n  Unused records: {unused_listed:,} from listed domains, \"\n",
        "              f\"{unused_other:,} from unlisted domains ({unused_listed + unused_other:,} total)\")\n",
        "else:\n",
        "    selected_records = records\n",
        "    print(\"No domain distribution specified - using all filtered records\")\n",
        "\n",
        "random.shuffle(selected_records)\n",
        "records = selected_records\n",
        "\n",
        "print(f\"\\nFinal dataset size: {len(records):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available records by dataset source (after previous filters):\n",
            "  gsm8k: 3,190\n",
            "  numina1.5:mixed: 1,128\n",
            "  open_math_reasoning: 947\n",
            "  numina1.5:metamath: 147\n",
            "  numina1.5:cn_contest: 49\n",
            "  numina1.5:aops: 37\n",
            "  numina1.5:number_theory: 7\n",
            "  numina1.5:inequalities: 4\n",
            "\n",
            "No dataset distribution specified - keeping all records\n",
            "Current dataset size: 5,509\n"
          ]
        }
      ],
      "source": [
        "by_dataset: dict[str, list] = {}\n",
        "for r in records:\n",
        "    ds = r.get(\"dataset\", \"unknown\")\n",
        "    by_dataset.setdefault(ds, []).append(r)\n",
        "\n",
        "print(\"Available records by dataset source (after previous filters):\")\n",
        "for ds, recs in sorted(by_dataset.items(), key=lambda x: -len(x[1])):\n",
        "    print(f\"  {ds}: {len(recs):,}\")\n",
        "\n",
        "if DATASET_DISTRIBUTION is not None:\n",
        "    total_pct = sum(DATASET_DISTRIBUTION.values())\n",
        "    if total_pct > 100:\n",
        "        raise ValueError(f\"Dataset percentages sum to {total_pct}%, must be <= 100%\")\n",
        "\n",
        "    pool_size = len(records)\n",
        "\n",
        "    max_proportional_targets = []\n",
        "    for ds_name, pct in DATASET_DISTRIBUTION.items():\n",
        "        available = len(by_dataset.get(ds_name, []))\n",
        "        if pct > 0:\n",
        "            max_for_ds = int(available / (pct / 100))\n",
        "            max_proportional_targets.append((ds_name, max_for_ds, available, pct))\n",
        "\n",
        "    max_proportional_targets.sort(key=lambda x: x[1])\n",
        "    bn_ds, max_achievable, bn_available, bn_pct = max_proportional_targets[0]\n",
        "\n",
        "    listed_available = sum(len(by_dataset.get(d, [])) for d in DATASET_DISTRIBUTION)\n",
        "    effective_target = min(pool_size, listed_available, max_achievable)\n",
        "\n",
        "    print(\"\\nDataset distribution analysis:\")\n",
        "    print(f\"  Pool size:              {pool_size:,}\")\n",
        "    print(f\"  Listed datasets total:  {listed_available:,}\")\n",
        "    print(f\"  Max proportional target: {max_achievable:,} (bottleneck: {bn_ds} \"\n",
        "          f\"with {bn_available:,} records @ {bn_pct}%)\")\n",
        "    print(f\"  Effective target:       {effective_target:,}\")\n",
        "\n",
        "    if effective_target < pool_size:\n",
        "        print(f\"\\n  NOTE: Capping at {effective_target:,} to maintain exact dataset proportions.\")\n",
        "\n",
        "    ds_allocated = {}\n",
        "    total_allocated = 0\n",
        "\n",
        "    print(f\"\\nAllocation (target: {effective_target:,}):\")\n",
        "    for ds_name, pct in DATASET_DISTRIBUTION.items():\n",
        "        available = by_dataset.get(ds_name, [])\n",
        "        desired = int(effective_target * pct / 100)\n",
        "        allocated = min(desired, len(available))\n",
        "        ds_allocated[ds_name] = allocated\n",
        "        total_allocated += allocated\n",
        "\n",
        "        actual_pct = (allocated / effective_target * 100) if effective_target > 0 else 0\n",
        "        tag = \" (ALL)\" if allocated == len(available) else \"\"\n",
        "        print(f\"  {ds_name}: {allocated:,} / {len(available):,} available{tag} \"\n",
        "              f\"[{actual_pct:.1f}% actual vs {pct}% desired]\")\n",
        "\n",
        "    remainder = effective_target - total_allocated\n",
        "    if remainder > 0:\n",
        "        for ds_name, pct in sorted(DATASET_DISTRIBUTION.items(), key=lambda x: -x[1]):\n",
        "            if remainder == 0:\n",
        "                break\n",
        "            available = len(by_dataset.get(ds_name, []))\n",
        "            capacity = available - ds_allocated[ds_name]\n",
        "            add = min(remainder, capacity)\n",
        "            if add > 0:\n",
        "                ds_allocated[ds_name] += add\n",
        "                total_allocated += add\n",
        "                remainder -= add\n",
        "\n",
        "    print(f\"\\n  Total allocated: {total_allocated:,} / {effective_target:,}\")\n",
        "\n",
        "    ds_selected = []\n",
        "    for ds_name, count in ds_allocated.items():\n",
        "        if count > 0:\n",
        "            available = by_dataset.get(ds_name, [])\n",
        "            if count >= len(available):\n",
        "                ds_selected.extend(available)\n",
        "            else:\n",
        "                ds_selected.extend(random.sample(available, count))\n",
        "\n",
        "    unused_listed = listed_available - total_allocated\n",
        "    unused_other = sum(len(v) for d, v in by_dataset.items() if d not in DATASET_DISTRIBUTION)\n",
        "    if unused_listed > 0 or unused_other > 0:\n",
        "        print(f\"\\n  Unused records: {unused_listed:,} from listed datasets, \"\n",
        "              f\"{unused_other:,} from unlisted datasets ({unused_listed + unused_other:,} total)\")\n",
        "\n",
        "    random.shuffle(ds_selected)\n",
        "    records = ds_selected\n",
        "    print(f\"\\nDataset size after dataset distribution: {len(records):,}\")\n",
        "else:\n",
        "    print(\"\\nNo dataset distribution specified - keeping all records\")\n",
        "    print(f\"Current dataset size: {len(records):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final dataset validation:\n",
            "  Target: 15000\n",
            "  Actual: 5,509\n",
            "  ⚠️  Shortfall: 9,491 records (36.7% of target)\n"
          ]
        }
      ],
      "source": [
        "print(\"Final dataset validation:\")\n",
        "print(f\"  Target: {TARGET_TOTAL_RECORDS or 'unlimited'}\")\n",
        "print(f\"  Actual: {len(records):,}\")\n",
        "\n",
        "if TARGET_TOTAL_RECORDS is not None:\n",
        "    if len(records) == TARGET_TOTAL_RECORDS:\n",
        "        print(\"  ✓ Target achieved\")\n",
        "    elif len(records) < TARGET_TOTAL_RECORDS:\n",
        "        shortfall = TARGET_TOTAL_RECORDS - len(records)\n",
        "        pct = (len(records) / TARGET_TOTAL_RECORDS) * 100\n",
        "        print(f\"  ⚠️  Shortfall: {shortfall:,} records ({pct:.1f}% of target)\")\n",
        "    else:\n",
        "        # This shouldn't happen with the new algorithm, but safety check\n",
        "        print(f\"  ⚠️  WARNING: Exceeded target by {len(records) - TARGET_TOTAL_RECORDS:,} records\")\n",
        "        print(\"     Downsampling to target size...\")\n",
        "        records = random.sample(records, TARGET_TOTAL_RECORDS)\n",
        "        print(f\"  ✓ Downsampled to {len(records):,} records\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stratification groups:\n",
            "  algebra_D2: 2,217\n",
            "  algebra_D3: 1,094\n",
            "  algebra_D4: 270\n",
            "  combinatorics_D2: 549\n",
            "  combinatorics_D3: 402\n",
            "  combinatorics_D4: 152\n",
            "  geometry_D2: 130\n",
            "  geometry_D3: 108\n",
            "  geometry_D4: 37\n",
            "  number_theory_D2: 303\n",
            "  number_theory_D3: 161\n",
            "  number_theory_D4: 86\n",
            "\n",
            "Train: 4,958\n",
            "Test:  551\n"
          ]
        }
      ],
      "source": [
        "# Build stratification keys based on domain + effective_difficulty + reasoning_depth\n",
        "strat_keys = []\n",
        "for r in records:\n",
        "    domain = get_domain(r)\n",
        "    eff_diff = r.get(\"_effective_difficulty\", \"NA\")\n",
        "    depth = r.get(\"_reasoning_depth\", \"unknown\")\n",
        "    strat_keys.append(f\"{domain}_D{eff_diff}_R{depth}\")\n",
        "\n",
        "print(\"Stratification groups:\")\n",
        "for k, v in sorted(Counter(strat_keys).items()):\n",
        "    print(f\"  {k}: {v:,}\")\n",
        "\n",
        "strat_counts = Counter(strat_keys)\n",
        "safe_keys = [\n",
        "    k if strat_counts[k] >= 2 else \"_rare_\"\n",
        "    for k in strat_keys\n",
        "]\n",
        "\n",
        "rare_count = sum(1 for k in safe_keys if k == \"_rare_\")\n",
        "if rare_count:\n",
        "    print(f\"\\nMerged {rare_count} record(s) from singleton strata into '_rare_' group\")\n",
        "\n",
        "indices = list(range(len(records)))\n",
        "\n",
        "train_idx, test_idx = train_test_split(\n",
        "    indices,\n",
        "    test_size=TEST_RATIO,\n",
        "    random_state=RANDOM_SEED,\n",
        "    stratify=safe_keys,\n",
        ")\n",
        "\n",
        "train_records = [records[i] for i in train_idx]\n",
        "test_records = [records[i] for i in test_idx]\n",
        "\n",
        "print(f\"\\nTrain: {len(train_records):,}\")\n",
        "print(f\"Test:  {len(test_records):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>group</th>\n",
              "      <th>total</th>\n",
              "      <th>train</th>\n",
              "      <th>test</th>\n",
              "      <th>train_%</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>algebra_D2</td>\n",
              "      <td>2217</td>\n",
              "      <td>1995</td>\n",
              "      <td>222</td>\n",
              "      <td>90.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>algebra_D3</td>\n",
              "      <td>1094</td>\n",
              "      <td>985</td>\n",
              "      <td>109</td>\n",
              "      <td>90.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>algebra_D4</td>\n",
              "      <td>270</td>\n",
              "      <td>243</td>\n",
              "      <td>27</td>\n",
              "      <td>90.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>combinatorics_D2</td>\n",
              "      <td>549</td>\n",
              "      <td>494</td>\n",
              "      <td>55</td>\n",
              "      <td>90.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>combinatorics_D3</td>\n",
              "      <td>402</td>\n",
              "      <td>362</td>\n",
              "      <td>40</td>\n",
              "      <td>90.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>combinatorics_D4</td>\n",
              "      <td>152</td>\n",
              "      <td>137</td>\n",
              "      <td>15</td>\n",
              "      <td>90.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>geometry_D2</td>\n",
              "      <td>130</td>\n",
              "      <td>117</td>\n",
              "      <td>13</td>\n",
              "      <td>90.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>geometry_D3</td>\n",
              "      <td>108</td>\n",
              "      <td>97</td>\n",
              "      <td>11</td>\n",
              "      <td>89.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>geometry_D4</td>\n",
              "      <td>37</td>\n",
              "      <td>33</td>\n",
              "      <td>4</td>\n",
              "      <td>89.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>number_theory_D2</td>\n",
              "      <td>303</td>\n",
              "      <td>273</td>\n",
              "      <td>30</td>\n",
              "      <td>90.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>number_theory_D3</td>\n",
              "      <td>161</td>\n",
              "      <td>145</td>\n",
              "      <td>16</td>\n",
              "      <td>90.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>number_theory_D4</td>\n",
              "      <td>86</td>\n",
              "      <td>77</td>\n",
              "      <td>9</td>\n",
              "      <td>89.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               group  total  train  test  train_%\n",
              "0         algebra_D2   2217   1995   222     90.0\n",
              "1         algebra_D3   1094    985   109     90.0\n",
              "2         algebra_D4    270    243    27     90.0\n",
              "3   combinatorics_D2    549    494    55     90.0\n",
              "4   combinatorics_D3    402    362    40     90.0\n",
              "5   combinatorics_D4    152    137    15     90.1\n",
              "6        geometry_D2    130    117    13     90.0\n",
              "7        geometry_D3    108     97    11     89.8\n",
              "8        geometry_D4     37     33     4     89.2\n",
              "9   number_theory_D2    303    273    30     90.1\n",
              "10  number_theory_D3    161    145    16     90.1\n",
              "11  number_theory_D4     86     77     9     89.5"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_strat = Counter(strat_keys[i] for i in train_idx)\n",
        "test_strat = Counter(strat_keys[i] for i in test_idx)\n",
        "all_groups = sorted(set(strat_keys))\n",
        "\n",
        "rows = []\n",
        "for g in all_groups:\n",
        "    total = train_strat.get(g, 0) + test_strat.get(g, 0)\n",
        "    rows.append({\n",
        "        \"group\": g,\n",
        "        \"total\": total,\n",
        "        \"train\": train_strat.get(g, 0),\n",
        "        \"test\": test_strat.get(g, 0),\n",
        "        \"train_%\": round(train_strat.get(g, 0) / total * 100, 1) if total else 0,\n",
        "    })\n",
        "\n",
        "pd.DataFrame(rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Samples per dataset:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dataset</th>\n",
              "      <th>total</th>\n",
              "      <th>train</th>\n",
              "      <th>test</th>\n",
              "      <th>pct_of_total</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>gsm8k</td>\n",
              "      <td>3190</td>\n",
              "      <td>2868</td>\n",
              "      <td>322</td>\n",
              "      <td>57.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>numina1.5:mixed</td>\n",
              "      <td>1128</td>\n",
              "      <td>1020</td>\n",
              "      <td>108</td>\n",
              "      <td>20.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>open_math_reasoning</td>\n",
              "      <td>947</td>\n",
              "      <td>850</td>\n",
              "      <td>97</td>\n",
              "      <td>17.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>numina1.5:metamath</td>\n",
              "      <td>147</td>\n",
              "      <td>132</td>\n",
              "      <td>15</td>\n",
              "      <td>2.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>numina1.5:cn_contest</td>\n",
              "      <td>49</td>\n",
              "      <td>44</td>\n",
              "      <td>5</td>\n",
              "      <td>0.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>numina1.5:aops</td>\n",
              "      <td>37</td>\n",
              "      <td>34</td>\n",
              "      <td>3</td>\n",
              "      <td>0.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>numina1.5:number_theory</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>numina1.5:inequalities</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   dataset  total  train  test  pct_of_total\n",
              "0                    gsm8k   3190   2868   322          57.9\n",
              "1          numina1.5:mixed   1128   1020   108          20.5\n",
              "2      open_math_reasoning    947    850    97          17.2\n",
              "3       numina1.5:metamath    147    132    15           2.7\n",
              "4     numina1.5:cn_contest     49     44     5           0.9\n",
              "5           numina1.5:aops     37     34     3           0.7\n",
              "6  numina1.5:number_theory      7      6     1           0.1\n",
              "7   numina1.5:inequalities      4      4     0           0.1"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_ds = Counter(r.get(\"dataset\", \"unknown\") for r in train_records)\n",
        "test_ds = Counter(r.get(\"dataset\", \"unknown\") for r in test_records)\n",
        "all_datasets = sorted(set(train_ds) | set(test_ds))\n",
        "\n",
        "ds_rows = []\n",
        "for ds in all_datasets:\n",
        "    total = train_ds.get(ds, 0) + test_ds.get(ds, 0)\n",
        "    ds_rows.append({\n",
        "        \"dataset\": ds,\n",
        "        \"total\": total,\n",
        "        \"train\": train_ds.get(ds, 0),\n",
        "        \"test\": test_ds.get(ds, 0),\n",
        "        \"pct_of_total\": round(total / len(records) * 100, 1),\n",
        "    })\n",
        "\n",
        "ds_rows.sort(key=lambda x: -x[\"total\"])\n",
        "\n",
        "print(\"Samples per dataset:\")\n",
        "pd.DataFrame(ds_rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved 4,958 train records -> /home/larcanio/AIMO3_v2/data/datasets/splits/gemma_balanced/train.jsonl\n",
            "Saved 551 test records  -> /home/larcanio/AIMO3_v2/data/datasets/splits/gemma_balanced/test.jsonl\n",
            "\n",
            "============================================================\n",
            "DATASET SUMMARY\n",
            "============================================================\n",
            "Sources: 87 datasets\n",
            "Filters: correct=True\n",
            "         effective_difficulty=[2, 3, 4]\n",
            "         code_scores=all\n",
            "         solution_tokens=all\n",
            "         max_constraint_count=1\n",
            "         max_object_count=1\n",
            "         reasoning_depth=['shallow', 'medium']\n",
            "Domains: algebra:65%, combinatorics:20%, geometry:5%, number_theory:10%\n",
            "Datasets: all (no distribution)\n",
            "Target:  15000\n",
            "Split:   90% train / 10% test\n",
            "Output:  /home/larcanio/AIMO3_v2/data/datasets/splits/gemma_balanced\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "train_path = os.path.join(OUTPUT_DIR, \"train.jsonl\")\n",
        "test_path = os.path.join(OUTPUT_DIR, \"test.jsonl\")\n",
        "\n",
        "with open(train_path, \"w\") as f:\n",
        "    for r in train_records:\n",
        "        f.write(json.dumps(r) + \"\\n\")\n",
        "\n",
        "with open(test_path, \"w\") as f:\n",
        "    for r in test_records:\n",
        "        f.write(json.dumps(r) + \"\\n\")\n",
        "\n",
        "print(f\"Saved {len(train_records):,} train records -> {train_path}\")\n",
        "print(f\"Saved {len(test_records):,} test records  -> {test_path}\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"DATASET SUMMARY\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Sources: {len(DATASET_PATH)} datasets\")\n",
        "print(f\"Filters: correct={REQUIRE_CORRECT}\")\n",
        "print(f\"         effective_difficulty={_fmt_pct_distribution(EFFECTIVE_DIFFICULTY)}\")\n",
        "print(f\"         code_scores={CODE_SCORES or 'all'}\")\n",
        "if SOLUTION_TOKEN_RANGE:\n",
        "    print(f\"         solution_tokens=[{SOLUTION_TOKEN_RANGE[0]}-{SOLUTION_TOKEN_RANGE[1]}]\")\n",
        "else:\n",
        "    print(\"         solution_tokens=all\")\n",
        "print(f\"         max_constraint_count={MAX_CONSTRAINT_COUNT or 'all'}\")\n",
        "print(f\"         max_object_count={MAX_OBJECT_COUNT or 'all'}\")\n",
        "print(f\"         reasoning_depth={_fmt_pct_distribution(REASONING_DEPTH)}\")\n",
        "if DOMAIN_DISTRIBUTION:\n",
        "    print(f\"Domains: {', '.join(f'{d}:{p}%' for d, p in DOMAIN_DISTRIBUTION.items())}\")\n",
        "else:\n",
        "    print(\"Domains: all (no distribution)\")\n",
        "if DATASET_DISTRIBUTION:\n",
        "    print(f\"Datasets: {', '.join(f'{d}:{p}%' for d, p in DATASET_DISTRIBUTION.items())}\")\n",
        "else:\n",
        "    print(\"Datasets: all (no distribution)\")\n",
        "print(f\"Target:  {TARGET_TOTAL_RECORDS or 'unlimited'}\")\n",
        "print(f\"Split:   {TRAIN_RATIO*100:.0f}% train / {TEST_RATIO*100:.0f}% test\")\n",
        "print(f\"Output:  {OUTPUT_DIR}\")\n",
        "print(f\"{'='*60}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
