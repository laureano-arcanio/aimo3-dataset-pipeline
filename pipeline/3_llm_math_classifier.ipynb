{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "imports",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import sys\n",
        "import re\n",
        "from pathlib import Path\n",
        "from tqdm.notebook import tqdm\n",
        "from datetime import datetime\n",
        "from typing import Dict, Any, Optional, List, Tuple\n",
        "import time\n",
        "\n",
        "import anyio\n",
        "import httpx\n",
        "from utils import LLMPool, RuntimeConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "config",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[config] reloaded: MAX_CONCURRENT_REQUESTS: 25 -> 20\n",
            "Configuration loaded:\n",
            "  Model: gpt-oss\n",
            "  Reasoning effort: low\n",
            "  Config file: config.json\n",
            "RuntimeConfig(LLM_REQUEST_RETRY_COUNT=3, LLM_REQUEST_TIMEOUT_SECONDS=300, MAX_CONCURRENT_REQUESTS=20)\n",
            "  Input file: /home/larcanio/AIMO3_v2/data/datasets/gsm8k_reasoning_steps/Compiled-GSM8K/dataset.jsonl\n",
            "  Output file: /home/larcanio/AIMO3_v2/data/datasets/gsm8k_reasoning_steps/Compiled-GSM8K/dataset_classfied.jsonl\n",
            "  Max problems: All\n"
          ]
        }
      ],
      "source": [
        "# Configuration Parameters\n",
        "\n",
        "# LLM Configuration\n",
        "API_KEY = \"sk-local\"\n",
        "MODEL_NAME = \"gpt-oss\"\n",
        "NGINX_BALANCER_URL = \"http://127.0.0.1:8080/v1\"\n",
        "\n",
        "TEMPERATURE = 0.3\n",
        "TOP_P = 0.95\n",
        "MAX_TOKENS = 1024 * 8\n",
        "REASONING_EFFORT = \"low\"\n",
        "\n",
        "# Input/Output Configuration\n",
        "INPUT_JSONL_PATH = Path(\"/home/larcanio/AIMO3_v2/data/datasets/gsm8k_reasoning_steps/Compiled-GSM8K/dataset.jsonl\")\n",
        "OUTPUT_JSONL_PATH = INPUT_JSONL_PATH.parent / \"dataset_classfied.jsonl\"\n",
        "\n",
        "# Processing Configuration\n",
        "MAX_PROBLEMS_TO_PROCESS = None\n",
        "START_FROM_INDEX = 0\n",
        "\n",
        "# Prompt File Paths\n",
        "PROMPT_DIR = Path(\"prompts\")\n",
        "TEXT_STRUCTURE_PROMPT_PATH = PROMPT_DIR / \"nb3_math_structure_from_text.md\"\n",
        "SOLUTION_STRUCTURE_PROMPT_PATH = PROMPT_DIR / \"nb3_math_structure_from_solution.md\"\n",
        "\n",
        "# Runtime Configuration\n",
        "CONFIG_FILE = \"config.json\"\n",
        "\n",
        "cfg = RuntimeConfig(CONFIG_FILE, defaults={\n",
        "    \"LLM_REQUEST_RETRY_COUNT\": 3,\n",
        "    \"LLM_REQUEST_TIMEOUT_SECONDS\": 300,\n",
        "    \"MAX_CONCURRENT_REQUESTS\": 25,\n",
        "})\n",
        "\n",
        "print(f\"Configuration: Model={MODEL_NAME}, Effort={REASONING_EFFORT}\")\n",
        "print(f\"Config: {cfg}\")\n",
        "print(f\"Input: {INPUT_JSONL_PATH.name}\")\n",
        "print(f\"Output: {OUTPUT_JSONL_PATH.name}\")\n",
        "print(f\"Max problems: {MAX_PROBLEMS_TO_PROCESS or 'All'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "prompts",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompts loaded\n",
            "  Text structure system prompt: 4404 chars\n",
            "  Solution structure system prompt: 4492 chars\n"
          ]
        }
      ],
      "source": [
        "# Prompt Templates\n",
        "\n",
        "# Load prompt templates from markdown files\n",
        "_text_prompt_raw = TEXT_STRUCTURE_PROMPT_PATH.read_text(encoding=\"utf-8\")\n",
        "_solution_prompt_raw = SOLUTION_STRUCTURE_PROMPT_PATH.read_text(encoding=\"utf-8\")\n",
        "\n",
        "# Split system prompt from trailing section header\n",
        "_text_split = _text_prompt_raw.split(\"### Problem\")\n",
        "TEXT_STRUCTURE_SYSTEM_PROMPT = _text_split[0].strip()\n",
        "\n",
        "_solution_split = _solution_prompt_raw.split(\"### Chain-of-Thought\")\n",
        "SOLUTION_STRUCTURE_SYSTEM_PROMPT = _solution_split[0].strip()\n",
        "\n",
        "# User prompt templates\n",
        "TEXT_STRUCTURE_USER_PROMPT = \"### Problem\\n\\n{PROBLEM_TEXT}\"\n",
        "SOLUTION_STRUCTURE_USER_PROMPT = \"### Chain-of-Thought\\n\\n{SOLUTION_CODE}\"\n",
        "\n",
        "print(f\"Prompts loaded: Text={len(TEXT_STRUCTURE_SYSTEM_PROMPT)} chars, \"\n",
        "      f\"Solution={len(SOLUTION_STRUCTURE_SYSTEM_PROMPT)} chars\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "client",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Helper functions loaded\n"
          ]
        }
      ],
      "source": [
        "# Helper Functions\n",
        "\n",
        "def parse_json_from_response(text: str) -> Dict[str, Any]:\n",
        "    \"\"\"Extract and parse JSON from LLM response.\n",
        "    \n",
        "    Handles markdown code blocks and surrounding text.\n",
        "    \"\"\"\n",
        "    if not text or text.strip() == \"\":\n",
        "        raise ValueError(\"Empty response from LLM\")\n",
        "    \n",
        "    text = text.strip()\n",
        "    if \"```\" in text:\n",
        "        code_block_pattern = r\"```(?:json)?\\s*\\n?(.*?)```\"\n",
        "        matches = re.findall(code_block_pattern, text, re.DOTALL)\n",
        "        if matches:\n",
        "            text = matches[0].strip()\n",
        "    \n",
        "    start_idx = text.find(\"{\")\n",
        "    if start_idx == -1:\n",
        "        raise ValueError(\"No JSON object found in response\")\n",
        "    \n",
        "    brace_count = 0\n",
        "    end_idx = start_idx\n",
        "    for i in range(start_idx, len(text)):\n",
        "        if text[i] == \"{\":\n",
        "            brace_count += 1\n",
        "        elif text[i] == \"}\":\n",
        "            brace_count -= 1\n",
        "            if brace_count == 0:\n",
        "                end_idx = i + 1\n",
        "                break\n",
        "    \n",
        "    json_str = text[start_idx:end_idx]\n",
        "    \n",
        "    try:\n",
        "        return json.loads(json_str)\n",
        "    except json.JSONDecodeError as e:\n",
        "        json_str_fixed = re.sub(r\",\\s*}\", \"}\", json_str)\n",
        "        json_str_fixed = re.sub(r\",\\s*]\", \"]\", json_str_fixed)\n",
        "        try:\n",
        "            return json.loads(json_str_fixed)\n",
        "        except:\n",
        "            raise ValueError(f\"Failed to parse JSON: {e}. Text: {json_str[:500]}\")\n",
        "\n",
        "print(\"Helper functions ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "load_data",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 7411 problems from /home/larcanio/AIMO3_v2/data/datasets/gsm8k_reasoning_steps/Compiled-GSM8K/dataset.jsonl\n",
            "Will process 7411 problems\n",
            "\n",
            "Existing math_structure coverage:\n",
            "  Both from_text + from_solution: 0 (will skip)\n",
            "  Only from_text:                 0 (will compute from_solution)\n",
            "  Only from_solution:             0 (will compute from_text)\n",
            "  Neither:                        7411 (will compute both)\n",
            "  Total needing work:             7411\n"
          ]
        }
      ],
      "source": [
        "# Load Data\n",
        "\n",
        "def load_problems_from_jsonl(jsonl_path: Path) -> list:\n",
        "    \"\"\"Load problems from JSONL file.\"\"\"\n",
        "    problems = []\n",
        "    with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
        "        for line_num, line in enumerate(f, 1):\n",
        "            if line.strip():\n",
        "                try:\n",
        "                    data = json.loads(line)\n",
        "                    problems.append(data)\n",
        "                except json.JSONDecodeError as e:\n",
        "                    print(f\"Error parsing line {line_num}: {e}\")\n",
        "                    continue\n",
        "    return problems\n",
        "\n",
        "all_problems = load_problems_from_jsonl(INPUT_JSONL_PATH)\n",
        "print(f\"Loaded {len(all_problems)} problems\")\n",
        "\n",
        "# Apply filters\n",
        "problems = all_problems[:]\n",
        "\n",
        "if START_FROM_INDEX > 0:\n",
        "    problems = problems[START_FROM_INDEX:]\n",
        "    print(f\"Starting from index {START_FROM_INDEX}\")\n",
        "\n",
        "if MAX_PROBLEMS_TO_PROCESS:\n",
        "    problems = problems[:MAX_PROBLEMS_TO_PROCESS]\n",
        "\n",
        "print(f\"Processing {len(problems)} problems\")\n",
        "\n",
        "# Position mapping for in-place update\n",
        "_id_to_full_pos = {id(dp): i for i, dp in enumerate(all_problems)}\n",
        "classify_to_full_pos = [_id_to_full_pos[id(dp)] for dp in problems]\n",
        "\n",
        "# Pre-scan existing math_structure data\n",
        "_pre_has_text = 0\n",
        "_pre_has_sol = 0\n",
        "_pre_has_both = 0\n",
        "_pre_has_neither = 0\n",
        "for dp in problems:\n",
        "    ms = dp.get(\"math_structure\", {})\n",
        "    has_text = bool(ms.get(\"from_text\"))\n",
        "    has_sol = bool(ms.get(\"from_solution\"))\n",
        "    if has_text and has_sol:\n",
        "        _pre_has_both += 1\n",
        "    elif has_text:\n",
        "        _pre_has_text += 1\n",
        "    elif has_sol:\n",
        "        _pre_has_sol += 1\n",
        "    else:\n",
        "        _pre_has_neither += 1\n",
        "\n",
        "print(f\"\\nExisting coverage:\")\n",
        "print(f\"  Both: {_pre_has_both} (skip) | Text only: {_pre_has_text} | Solution only: {_pre_has_sol}\")\n",
        "print(f\"  Neither: {_pre_has_neither} | Total needing work: {len(problems) - _pre_has_both}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "process",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ae3b1f3136d34aebabc16e197ca87f44",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Extracting:   0%|          | 0/7411 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[checkpoint 1] 500/7411 saved\n",
            "\n",
            "[checkpoint 2] 1000/7411 saved\n",
            "\n",
            "[checkpoint 3] 1500/7411 saved\n",
            "\n",
            "[checkpoint 4] 2000/7411 saved\n",
            "\n",
            "[checkpoint 5] 2500/7411 saved\n",
            "\n",
            "[checkpoint 6] 3000/7411 saved\n",
            "\n",
            "[checkpoint 7] 3500/7411 saved\n",
            "\n",
            "[checkpoint 8] 4000/7411 saved\n",
            "\n",
            "[checkpoint 9] 4500/7411 saved\n",
            "\n",
            "[checkpoint 10] 5000/7411 saved\n",
            "\n",
            "[checkpoint 11] 5500/7411 saved\n",
            "[config] reloaded: MAX_CONCURRENT_REQUESTS: 20 -> 30\n",
            "\n",
            "[checkpoint 12] 6000/7411 saved\n",
            "\n",
            "[checkpoint 13] 6500/7411 saved\n",
            "[config] reloaded: MAX_CONCURRENT_REQUESTS: 30 -> 50\n",
            "\n",
            "[checkpoint 14] 7000/7411 saved\n",
            "\n",
            "[checkpoint 15] 7411/7411 saved\n",
            "\n",
            "Processed 7411 problems (0 already complete, 0 skipped, 7411 extracted)\n",
            "Total tokens: in=17,067,645, out=2,936,974, total=20,004,619\n",
            "\n",
            "Extraction Completeness:\n",
            "  Both text + solution: 7317\n",
            "  Text only:           94\n",
            "  Solution only:       0\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# PROCESS PROBLEMS - TWO-PASS STRUCTURE EXTRACTION\n",
        "# ============================================================================\n",
        "\n",
        "SAVE_EVERY = 500\n",
        "\n",
        "\n",
        "async def extract_text_structure(pool: LLMPool, problem_text: str) -> Tuple[Optional[Dict[str, Any]], int, int]:\n",
        "    \"\"\"Extract mathematical structure from the problem text.\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of (structure_dict, prompt_tokens, completion_tokens)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        user_prompt = TEXT_STRUCTURE_USER_PROMPT.format(PROBLEM_TEXT=problem_text)\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": TEXT_STRUCTURE_SYSTEM_PROMPT},\n",
        "            {\"role\": \"user\", \"content\": user_prompt},\n",
        "        ]\n",
        "        resp = await pool.request(\n",
        "            messages,\n",
        "            temperature=TEMPERATURE,\n",
        "            max_tokens=MAX_TOKENS,\n",
        "            reasoning_effort=REASONING_EFFORT,\n",
        "            top_p=TOP_P,\n",
        "        )\n",
        "        if not resp.content:\n",
        "            return None, resp.prompt_tokens, resp.completion_tokens\n",
        "        return parse_json_from_response(resp.content), resp.prompt_tokens, resp.completion_tokens\n",
        "    except Exception as e:\n",
        "        print(f\"  text-structure error: {type(e).__name__}: {str(e)[:100]}\")\n",
        "        return None, 0, 0\n",
        "\n",
        "\n",
        "async def extract_solution_structure(pool: LLMPool, solution_code: str) -> Tuple[Optional[Dict[str, Any]], int, int]:\n",
        "    \"\"\"Extract reasoning structure from the solution code / CoT.\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of (structure_dict, prompt_tokens, completion_tokens)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        user_prompt = SOLUTION_STRUCTURE_USER_PROMPT.format(SOLUTION_CODE=solution_code)\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": SOLUTION_STRUCTURE_SYSTEM_PROMPT},\n",
        "            {\"role\": \"user\", \"content\": user_prompt},\n",
        "        ]\n",
        "        resp = await pool.request(\n",
        "            messages,\n",
        "            temperature=TEMPERATURE,\n",
        "            max_tokens=MAX_TOKENS,\n",
        "            reasoning_effort=REASONING_EFFORT,\n",
        "            top_p=TOP_P,\n",
        "        )\n",
        "        if not resp.content:\n",
        "            return None, resp.prompt_tokens, resp.completion_tokens\n",
        "        return parse_json_from_response(resp.content), resp.prompt_tokens, resp.completion_tokens\n",
        "    except Exception as e:\n",
        "        print(f\"  solution-structure error: {type(e).__name__}: {str(e)[:100]}\")\n",
        "        return None, 0, 0\n",
        "\n",
        "\n",
        "def _save_checkpoint(completed_n, total_n, save_num):\n",
        "    \"\"\"Write all_problems to disk atomically (runs in thread).\"\"\"\n",
        "    OUTPUT_JSONL_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
        "    tmp = OUTPUT_JSONL_PATH.with_suffix('.tmp')\n",
        "    with open(tmp, 'w', encoding='utf-8') as f:\n",
        "        for dp in all_problems:\n",
        "            f.write(json.dumps(dp, ensure_ascii=False) + '\\n')\n",
        "    tmp.rename(OUTPUT_JSONL_PATH)\n",
        "    print(f\"\\n[checkpoint {save_num}] {completed_n}/{total_n} saved\")\n",
        "\n",
        "\n",
        "async def run_all():\n",
        "    \"\"\"Run the two-pass extraction pipeline.\"\"\"\n",
        "    results = []\n",
        "    completed = 0\n",
        "    save_count = 0\n",
        "    total = len(problems)\n",
        "\n",
        "    write_lock = anyio.Lock()\n",
        "    save_lock = anyio.Lock()\n",
        "    counter = {\n",
        "        \"done\": 0, \"skipped\": 0, \"already_complete\": 0,\n",
        "        \"tok_in\": 0, \"tok_out\": 0,\n",
        "        \"both\": 0, \"text_only\": 0, \"solution_only\": 0,\n",
        "    }\n",
        "\n",
        "    pbar = tqdm(total=total, desc=\"Extracting\")\n",
        "    spawn_limit = anyio.Semaphore(cfg.MAX_CONCURRENT_REQUESTS * 3)\n",
        "\n",
        "    async def config_reloader(pool):\n",
        "        \"\"\"Re-read config.json every 60s and apply changes to the pool.\"\"\"\n",
        "        while True:\n",
        "            await anyio.sleep(60)\n",
        "            cfg.reload()\n",
        "            pool._limiter.total_tokens = cfg.MAX_CONCURRENT_REQUESTS\n",
        "            pool.max_retries = cfg.LLM_REQUEST_RETRY_COUNT\n",
        "            pool._client.timeout = httpx.Timeout(cfg.LLM_REQUEST_TIMEOUT_SECONDS, connect=30)\n",
        "\n",
        "    async def process_one(pool, idx, dp):\n",
        "        nonlocal completed, save_count\n",
        "\n",
        "        tok_in = 0\n",
        "        tok_out = 0\n",
        "\n",
        "        problem_text = dp.get(\"problem\", {}).get(\"text\", \"\")\n",
        "        if not problem_text:\n",
        "            print(f\"  Problem {idx} has no text, skipping\")\n",
        "            async with write_lock:\n",
        "                counter[\"done\"] += 1\n",
        "                counter[\"skipped\"] += 1\n",
        "                completed += 1\n",
        "                pbar.update(1)\n",
        "            return\n",
        "\n",
        "        # Resolve solution code from pass_at_k\n",
        "        pass_at_k = dp.get(\"outcome\", {}).get(\"pass_at_k\") or 0\n",
        "        attempts = dp.get(\"attempts\", [])\n",
        "        solution_code = \"\"\n",
        "        if pass_at_k > 0 and pass_at_k <= len(attempts):\n",
        "            solution_code = attempts[pass_at_k - 1].get(\"code\", \"\")\n",
        "\n",
        "        # Check existing math_structure to avoid redundant computation\n",
        "        existing_ms = dp.get(\"math_structure\", {})\n",
        "        existing_text = existing_ms.get(\"from_text\")\n",
        "        existing_sol = existing_ms.get(\"from_solution\")\n",
        "        need_text = not existing_text\n",
        "        need_sol = not existing_sol\n",
        "\n",
        "        if not need_text and not need_sol:\n",
        "            # Both already present, skip entirely\n",
        "            async with write_lock:\n",
        "                counter[\"done\"] += 1\n",
        "                counter[\"already_complete\"] += 1\n",
        "                completed += 1\n",
        "                pbar.update(1)\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            t0 = time.monotonic()\n",
        "\n",
        "            # --- Pass 1: text structure (only if missing) ---\n",
        "            text_struct = existing_text\n",
        "            if need_text:\n",
        "                text_struct, p1_in, p1_out = await extract_text_structure(pool, problem_text)\n",
        "                tok_in += p1_in\n",
        "                tok_out += p1_out\n",
        "\n",
        "            # --- Pass 2: solution structure (only if missing and code exists) ---\n",
        "            solution_struct = existing_sol\n",
        "            if need_sol:\n",
        "                if solution_code:\n",
        "                    solution_struct, p2_in, p2_out = await extract_solution_structure(pool, solution_code)\n",
        "                    tok_in += p2_in\n",
        "                    tok_out += p2_out\n",
        "                else:\n",
        "                    solution_struct = None\n",
        "\n",
        "            elapsed = time.monotonic() - t0\n",
        "\n",
        "            if text_struct is None and solution_struct is None:\n",
        "                print(f\"  Both extractions failed for problem {idx}\")\n",
        "                async with write_lock:\n",
        "                    counter[\"done\"] += 1\n",
        "                    counter[\"tok_in\"] += tok_in\n",
        "                    counter[\"tok_out\"] += tok_out\n",
        "                    completed += 1\n",
        "                    pbar.update(1)\n",
        "                return\n",
        "\n",
        "            # Build result\n",
        "            result = dict(dp)\n",
        "            result[\"math_structure\"] = {\n",
        "                \"from_text\": text_struct,\n",
        "                \"from_solution\": solution_struct,\n",
        "            }\n",
        "            result[\"extraction_timestamp\"] = datetime.now().isoformat()\n",
        "            result[\"extraction_model\"] = MODEL_NAME\n",
        "            result[\"extraction_reasoning_effort\"] = REASONING_EFFORT\n",
        "            result[\"extraction_tokens\"] = {\"prompt\": tok_in, \"completion\": tok_out}\n",
        "\n",
        "            should_save = False\n",
        "            async with write_lock:\n",
        "                results.append(result)\n",
        "                all_problems[classify_to_full_pos[idx]] = result\n",
        "                counter[\"done\"] += 1\n",
        "                counter[\"tok_in\"] += tok_in\n",
        "                counter[\"tok_out\"] += tok_out\n",
        "                completed += 1\n",
        "\n",
        "                done = counter[\"done\"]\n",
        "                skipped = counter[\"skipped\"]\n",
        "                already = counter[\"already_complete\"]\n",
        "                extracted = done - skipped - already\n",
        "                has_both = counter[\"both\"]\n",
        "                text_only = counter[\"text_only\"]\n",
        "                pbar.set_description(\n",
        "                    f\"Extracting ({extracted} done, {has_both} both, {text_only} text-only) [skip:{skipped}, cached:{already}]\"\n",
        "                )\n",
        "                pbar.update(1)\n",
        "\n",
        "                # Track extraction completeness\n",
        "                if text_struct and solution_struct:\n",
        "                    counter[\"both\"] += 1\n",
        "                elif text_struct:\n",
        "                    counter[\"text_only\"] += 1\n",
        "                else:\n",
        "                    counter[\"solution_only\"] += 1\n",
        "\n",
        "                should_save = completed % SAVE_EVERY == 0\n",
        "\n",
        "            if should_save:\n",
        "                async with save_lock:\n",
        "                    save_count += 1\n",
        "                    await anyio.to_thread.run_sync(\n",
        "                        _save_checkpoint, completed, total, save_count,\n",
        "                    )\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  Failed to extract problem {idx}: {type(e).__name__}: {str(e)[:100]}\")\n",
        "            async with write_lock:\n",
        "                counter[\"done\"] += 1\n",
        "                counter[\"tok_in\"] += tok_in\n",
        "                counter[\"tok_out\"] += tok_out\n",
        "                completed += 1\n",
        "                pbar.update(1)\n",
        "\n",
        "    async with LLMPool(\n",
        "        base_url=NGINX_BALANCER_URL,\n",
        "        api_key=API_KEY,\n",
        "        model=MODEL_NAME,\n",
        "        max_inflight=cfg.MAX_CONCURRENT_REQUESTS,\n",
        "        timeout=cfg.LLM_REQUEST_TIMEOUT_SECONDS,\n",
        "        max_retries=cfg.LLM_REQUEST_RETRY_COUNT,\n",
        "    ) as pool:\n",
        "        async with anyio.create_task_group() as tg:\n",
        "            tg.start_soon(config_reloader, pool)\n",
        "            async with anyio.create_task_group() as work_tg:\n",
        "                for idx, dp in enumerate(problems):\n",
        "                    await spawn_limit.acquire()\n",
        "                    async def _run(pool=pool, idx=idx, dp=dp):\n",
        "                        try:\n",
        "                            await process_one(pool, idx, dp)\n",
        "                        finally:\n",
        "                            spawn_limit.release()\n",
        "                    work_tg.start_soon(_run)\n",
        "            tg.cancel_scope.cancel()\n",
        "\n",
        "    pbar.close()\n",
        "\n",
        "    # Final save\n",
        "    save_count += 1\n",
        "    _save_checkpoint(completed, total, save_count)\n",
        "\n",
        "    total_tok_in = counter[\"tok_in\"]\n",
        "    total_tok_out = counter[\"tok_out\"]\n",
        "    total_tok = total_tok_in + total_tok_out\n",
        "\n",
        "    print(f\"\\nProcessed {counter['done']} problems ({counter['already_complete']} already complete, {counter['skipped']} skipped, {len(results)} extracted)\")\n",
        "    print(f\"Total tokens: in={total_tok_in:,}, out={total_tok_out:,}, total={total_tok:,}\")\n",
        "    print(f\"\\nExtraction Completeness:\")\n",
        "    print(f\"  Both text + solution: {counter['both']}\")\n",
        "    print(f\"  Text only:           {counter['text_only']}\")\n",
        "    print(f\"  Solution only:       {counter['solution_only']}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Run the processing\n",
        "results = await run_all()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "save",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "All 7411 records saved to /home/larcanio/AIMO3_v2/data/datasets/gsm8k_reasoning_steps/Compiled-GSM8K/dataset_classfied.jsonl\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# SAVE RESULTS (atomic final write of full dataset)\n",
        "# ============================================================================\n",
        "\n",
        "# Results are already saved incrementally during processing via checkpoints.\n",
        "# Do a final atomic write to ensure everything is persisted.\n",
        "OUTPUT_JSONL_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
        "tmp = OUTPUT_JSONL_PATH.with_suffix('.tmp')\n",
        "with open(tmp, 'w', encoding='utf-8') as f:\n",
        "    for dp in all_problems:\n",
        "        f.write(json.dumps(dp, ensure_ascii=False) + '\\n')\n",
        "tmp.rename(OUTPUT_JSONL_PATH)\n",
        "print(f\"\\nAll {len(all_problems)} records saved to {OUTPUT_JSONL_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "stats",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "TEXT STRUCTURE (from_text)\n",
            "======================================================================\n",
            "\n",
            "Domain Distribution:\n",
            "domain\n",
            "algebra          5476\n",
            "combinatorics     892\n",
            "number_theory     863\n",
            "geometry          158\n",
            "mixed              21\n",
            "set                 1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Output Type Distribution:\n",
            "output_type\n",
            "exact_value       7315\n",
            "maximum             44\n",
            "NaN                 23\n",
            "minimum             16\n",
            "existence           11\n",
            "non_existence        1\n",
            "classification       1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Top Objects:\n",
            "integer             6187\n",
            "real                1983\n",
            "positive_integer    1195\n",
            "rational              44\n",
            "sequence              35\n",
            "set                   27\n",
            "polygon               20\n",
            "triangle              10\n",
            "point                  6\n",
            "rectangle              6\n",
            "dtype: int64\n",
            "\n",
            "Top Constraints:\n",
            "equality            5608\n",
            "exists               638\n",
            "inequality           266\n",
            "divisibility         256\n",
            "bounded               86\n",
            "integral              70\n",
            "distinct              43\n",
            "forall                31\n",
            "positive_integer       7\n",
            "integer                5\n",
            "dtype: int64\n",
            "\n",
            "Top Mechanisms:\n",
            "  (none extracted)\n",
            "\n",
            "======================================================================\n",
            "SOLUTION STRUCTURE (from_solution)  [7097/7411 have data]\n",
            "======================================================================\n",
            "\n",
            "reasoning_shape:\n",
            "reasoning_shape\n",
            "linear    7095\n",
            "null         2\n",
            "Name: count, dtype: int64\n",
            "\n",
            "case_split:\n",
            "case_split\n",
            "none      7084\n",
            "binary      12\n",
            "null         1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "invariant:\n",
            "invariant\n",
            "none        7095\n",
            "null           1\n",
            "implicit       1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "auxiliary_construction:\n",
            "auxiliary_construction\n",
            "symbolic      4946\n",
            "none          2145\n",
            "structural       5\n",
            "null             1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "reasoning_depth:\n",
            "reasoning_depth\n",
            "shallow    3698\n",
            "medium     3063\n",
            "deep        334\n",
            "null          2\n",
            "Name: count, dtype: int64\n",
            "\n",
            "technique_transitions:\n",
            "technique_transitions\n",
            "0       7010\n",
            "1         78\n",
            "2          8\n",
            "null       1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "argument_style:\n",
            "argument_style\n",
            "direct    6647\n",
            "NaN        424\n",
            "none        23\n",
            "null         3\n",
            "Name: count, dtype: int64\n",
            "\n",
            "reasoning_scope:\n",
            "reasoning_scope\n",
            "local     7094\n",
            "null         1\n",
            "global       1\n",
            "NaN          1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "dead_end_pruning:\n",
            "dead_end_pruning\n",
            "False    7096\n",
            "null        1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "intermediate_reuse:\n",
            "intermediate_reuse\n",
            "single      2874\n",
            "none        2639\n",
            "multiple    1583\n",
            "null           1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "======================================================================\n",
            "COMBINED SUMMARY\n",
            "======================================================================\n",
            "  Total results:            7411\n",
            "  With from_text:           7411\n",
            "  With from_solution:       7097\n",
            "  With both:                7097\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# STATISTICS\n",
        "# ============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "if not results:\n",
        "    print(\"No results to analyze\")\n",
        "else:\n",
        "    # --- from_text fields ---\n",
        "    text_rows = []\n",
        "    for r in results:\n",
        "        ft = r.get(\"math_structure\", {}).get(\"from_text\") or {}\n",
        "        text_rows.append({\n",
        "            \"problem_id\": r.get(\"problem_id\", \"\"),\n",
        "            \"domain\": ft.get(\"domain\"),\n",
        "            \"output_type\": ft.get(\"output_type\"),\n",
        "            \"objects\": ft.get(\"objects\", []),\n",
        "            \"constraints\": ft.get(\"constraints\", []),\n",
        "            \"mechanisms\": ft.get(\"mechanisms\", []),\n",
        "            \"n_objects\": len(ft.get(\"objects\", [])),\n",
        "            \"n_constraints\": len(ft.get(\"constraints\", [])),\n",
        "            \"n_mechanisms\": len(ft.get(\"mechanisms\", [])),\n",
        "        })\n",
        "    df_text = pd.DataFrame(text_rows)\n",
        "\n",
        "    # --- from_solution fields ---\n",
        "    sol_rows = []\n",
        "    for r in results:\n",
        "        fs = r.get(\"math_structure\", {}).get(\"from_solution\") or {}\n",
        "        sol_rows.append({\n",
        "            \"problem_id\": r.get(\"problem_id\", \"\"),\n",
        "            \"reasoning_shape\": fs.get(\"reasoning_shape\"),\n",
        "            \"case_split\": fs.get(\"case_split\"),\n",
        "            \"invariant\": fs.get(\"invariant\"),\n",
        "            \"auxiliary_construction\": fs.get(\"auxiliary_construction\"),\n",
        "            \"reasoning_depth\": fs.get(\"reasoning_depth\"),\n",
        "            \"technique_transitions\": fs.get(\"technique_transitions\"),\n",
        "            \"argument_style\": fs.get(\"argument_style\"),\n",
        "            \"reasoning_scope\": fs.get(\"reasoning_scope\"),\n",
        "            \"dead_end_pruning\": fs.get(\"dead_end_pruning\"),\n",
        "            \"intermediate_reuse\": fs.get(\"intermediate_reuse\"),\n",
        "        })\n",
        "    df_sol = pd.DataFrame(sol_rows)\n",
        "\n",
        "    print(\"=\" * 70)\n",
        "    print(\"TEXT STRUCTURE (from_text)\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    print(\"\\nDomain Distribution:\")\n",
        "    print(df_text[\"domain\"].value_counts())\n",
        "\n",
        "    print(\"\\nOutput Type Distribution:\")\n",
        "    print(df_text[\"output_type\"].value_counts(dropna=False))\n",
        "\n",
        "    # Flatten array fields\n",
        "    all_objects = [o for objs in df_text[\"objects\"] for o in (objs if isinstance(objs, list) else [])]\n",
        "    all_constraints = [c for cs in df_text[\"constraints\"] for c in (cs if isinstance(cs, list) else [])]\n",
        "    all_mechanisms = [m for ms in df_text[\"mechanisms\"] for m in (ms if isinstance(ms, list) else [])]\n",
        "\n",
        "    print(\"\\nTop Objects:\")\n",
        "    print(pd.Series(Counter(all_objects)).sort_values(ascending=False).head(10))\n",
        "\n",
        "    print(\"\\nTop Constraints:\")\n",
        "    print(pd.Series(Counter(all_constraints)).sort_values(ascending=False).head(10))\n",
        "\n",
        "    print(\"\\nTop Mechanisms:\")\n",
        "    if all_mechanisms:\n",
        "        print(pd.Series(Counter(all_mechanisms)).sort_values(ascending=False).head(10))\n",
        "    else:\n",
        "        print(\"  (none extracted)\")\n",
        "\n",
        "    has_solution = df_sol[\"reasoning_shape\"].notna().sum()\n",
        "    print(f\"\\n{'=' * 70}\")\n",
        "    print(f\"SOLUTION STRUCTURE (from_solution)  [{has_solution}/{len(df_sol)} have data]\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    if has_solution > 0:\n",
        "        df_s = df_sol.dropna(subset=[\"reasoning_shape\"])\n",
        "\n",
        "        for col in [\"reasoning_shape\", \"case_split\", \"invariant\", \"auxiliary_construction\",\n",
        "                     \"reasoning_depth\", \"technique_transitions\", \"argument_style\",\n",
        "                     \"reasoning_scope\", \"dead_end_pruning\", \"intermediate_reuse\"]:\n",
        "            print(f\"\\n{col}:\")\n",
        "            print(df_s[col].value_counts(dropna=False))\n",
        "\n",
        "    # Combined summary\n",
        "    print(f\"\\n{'=' * 70}\")\n",
        "    print(\"COMBINED SUMMARY\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"  Total results:            {len(results)}\")\n",
        "    print(f\"  With from_text:           {df_text['domain'].notna().sum()}\")\n",
        "    print(f\"  With from_solution:       {has_solution}\")\n",
        "    print(f\"  With both:                {(df_text['domain'].notna() & df_sol['reasoning_shape'].notna()).sum()}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
